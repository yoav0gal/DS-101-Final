{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4 - Building a Predictive Model for Hotel Pricing Dynamics Using Snapshot Data (V2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Objective:**  \n",
    "  Predict the `Discount Price` for check-in dates within 30 days of a given `Snapshot Date` while minimizing the number of price samples (queries) from the database.\n",
    "\n",
    "- **Context:**  \n",
    "  Given fields of `Snapshot Date`, `Hotel Name`, `Discount Code`, build a relevant data set to predict the `Discount Price` for check-in dates within 30 days of the `Snapshot Date`.\n",
    "\n",
    "- **Performance Measure:**  \n",
    "  The model's success is evaluated by the R-squared metric on a subset of 40 hotels with the most complete data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Approaches: Gaussian Process Regression vs. Bayesian Linear and Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gaussian Process Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - **Uncertainty Quantification:** Provides both predictions and confidence intervals, which is key for active sampling.\n",
    "  - **Sample Efficiency:** Excels when data is limited, helping us select the most informative samples.\n",
    "  - **Flexibility:** Capable of modeling complex, non-linear relationships inherent in price trends.\n",
    "  \n",
    "- **Cons:**\n",
    "  - **Computational Cost:** Can become computationally expensive with larger datasets.\n",
    "  - **Kernel Selection:** Requires careful tuning of the kernel functions to capture the underlying data patterns accurately.\n",
    "\n",
    "- **Fit to the Problem:**  \n",
    "  Ideal for our task since its uncertainty estimates allow us to strategically choose the next check-in dates to sample, ensuring we use as few queries as possible while maintaining high prediction performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Bayesian Linear and Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - **Simplicity:** Easy to implement and interpret.\n",
    "  - **Speed:** Generally faster to train compared to GP, which can be advantageous with simpler or smooth trends.\n",
    "  \n",
    "- **Cons:**\n",
    "  - **Limited Flexibility:** May not capture complex non-linear patterns as effectively.\n",
    "  - **Uncertainty Estimates:** While Bayesian methods provide uncertainty measures, they are often less robust in capturing the variability in complex datasets compared to GP.\n",
    "\n",
    "- **Fit to the Problem:**  \n",
    "  Suitable if the price trends were very smooth and predictable. However, the nuances in hotel pricing (e.g., varying discount strategies, day-of-week effects) suggest a need for a more flexible approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Chosen Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Gaussian Process Regression** is selected as the primary method. Its ability to provide uncertainty estimates and guide active sampling makes it the best fit for minimizing the number of price queries while still achieving a high R-squared on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the Data:**  \n",
    "   We read the CSV file into a DataFrame and ensure that date columns are properly converted to datetime objects.\n",
    "\n",
    "2. **Filter the Data:**  \n",
    "   We filter the dataset to include only rows where `DayDiff` is 30 or less, meaning the check-in is within 30 days after the snapshot date.\n",
    "\n",
    "3. **Group the Data:**  \n",
    "   We group by `Snapshot Date`, `Hotel Name`, and `Discount Code`, and count the number of transactions (rows) in each group. This count is stored in the column `transaction_count`.\n",
    "\n",
    "4. **Dataset: onePerHotel:**  \n",
    "   - For each hotel, we sort by `transaction_count` in descending order and select the combination with the maximum count.  \n",
    "   - Then, we sort these unique hotel rows by `transaction_count` and take the top 40.\n",
    "\n",
    "5. **Dataset: mostData:**  \n",
    "   - We simply sort all the grouped combinations by `transaction_count` in descending order and select the top 40 combinations.\n",
    "\n",
    "6. **Extracting Parameters:**  \n",
    "   For each dataset, we extract a list of dictionaries (or \"params\") containing `Snapshot Date`, `Hotel Name`, and `Discount Code`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"./hotels_data_changed.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# We are only interested in transactions with a check-in date within 30 days after the snapshot.\n",
    "data_filtered = data[data['DayDiff'] <= 30].copy()\n",
    "\n",
    "data_filtered['Snapshot Date'] = pd.to_datetime(data_filtered['Snapshot Date']).dt.normalize()\n",
    "\n",
    "# We group by the combination of 'Snapshot Date', 'Hotel Name', and 'Discount Code', and count the number of transactions.\n",
    "grouped = data_filtered.groupby(\n",
    "    ['Snapshot Date', 'Hotel Name', 'Discount Code']\n",
    ").size().reset_index(name='transaction_count')\n",
    "\n",
    "# For each hotel, choose the combination with the highest transaction_count.\n",
    "# First, sort within each hotel so that the highest count is on top,\n",
    "# then group by 'Hotel Name' and take the first (best) row.\n",
    "onePerHotel = (\n",
    "    grouped.sort_values(['Hotel Name', 'transaction_count'], ascending=[True, False])\n",
    "    .groupby('Hotel Name')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "onePerHotel = onePerHotel.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "mostData = grouped.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "params_onePerHotel = onePerHotel[['Snapshot Date', 'Hotel Name', 'Discount Code']].to_dict('records')\n",
    "params_mostData   = mostData[['Snapshot Date', 'Hotel Name', 'Discount Code']].to_dict('records')\n",
    "\n",
    "print(\"onePerHotel (Top 40 hotels with best combination per hotel):\")\n",
    "display(onePerHotel.head())\n",
    "\n",
    "print(\"mostData (Top 40 combinations overall):\")\n",
    "display(mostData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we would create a function that gets the `Snapshot Date`, `Hotel Name`, and `Discount Code` and returns all we need to start training the model.\n",
    "This includes the following:\n",
    "-  get relevant data from the dataset\n",
    "-  apply feature engineering on that data\n",
    "-  normalize and scale the data\n",
    "-  Splitting into X and y & Creating the Wrapper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_data(params, data):\n",
    " \n",
    "    snapshot_date = pd.to_datetime(params['Snapshot Date']).normalize()\n",
    "    \n",
    "    subset = data[\n",
    "        (data['Snapshot Date'] == snapshot_date) &\n",
    "        (data['Hotel Name'] == params['Hotel Name']) &\n",
    "        (data['Discount Code'] == params['Discount Code']) &\n",
    "        (data['DayDiff'] <= 30) \n",
    "    ]\n",
    "    return subset\n",
    "\n",
    "test_params = params_mostData[0]\n",
    "relevant_subset = load_relevant_data(test_params, data_filtered)\n",
    "print(\"Parameters:\", test_params)\n",
    "print(\"Relevant Data (first 5 rows):\")\n",
    "display(relevant_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we transform our raw data into a format that is ready for model training. The goals are to:\n",
    "\n",
    "- **Convert Date Information:**  \n",
    "  We convert the date column `Checkin Date` into numerical values (Unix timestamps) so that the model can understand and work with temporal data.\n",
    "\n",
    "- **Encode Weekday Information:**  \n",
    "  We convert the weekday from a string (e.g., \"Monday\") into a numeric value using the helper function `convert_weekday_to_num`, storing it as `WeekDay_num`.\n",
    "\n",
    "- **Remove Irrelevant or Constant Columns:**  \n",
    "  After feature engineering, some columns become redundant or uninformative:\n",
    "  - **`Hotel Name`:** Removed because the data is filtered for a specific hotel, or if used for filtering, the raw name is no longer needed.\n",
    "  - **`Checkin Date`:** Once their numerical representations are extracted, the raw date values are no longer required.\n",
    "  - **`WeekDay`:** We already have the numeric `WeekDay_num`, making the original string column redundant.\n",
    "  - **`Days`:** This column is always the same (always 5), so it does not provide any variance or useful information for training.\n",
    "  - **`Snapshot Date`:** Since the snapshot is the same across the subset, this column is constant and does not help differentiate between records.\n",
    "  - **`Snapshot ID`:** This is also constant for a given snapshot and can be removed to reduce noise.\n",
    "\n",
    "The removal of these columns helps to simplify our dataset and ensures that the model is trained only on features that provide meaningful variation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weekday_to_num(weekday):\n",
    "    mapping = {\n",
    "        'Monday': 0,\n",
    "        'Tuesday': 1,\n",
    "        'Wednesday': 2,\n",
    "        'Thursday': 3,\n",
    "        'Friday': 4,\n",
    "        'Saturday': 5,\n",
    "        'Sunday': 6\n",
    "    }\n",
    "    return mapping.get(weekday, -1)  # Returns -1 if the weekday is not found\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df['Checkin Date'] = pd.to_datetime(df['Checkin Date'])\n",
    "    \n",
    "    df['Checkin_Date_num'] = df['Checkin Date'].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    \n",
    "    df['WeekDay_num'] = df['WeekDay'].apply(convert_weekday_to_num)\n",
    "    \n",
    "    # Remove columns that are not useful for training:\n",
    "    # 'Hotel Name'   - No longer needed since the data is already filtered per hotel.\n",
    "    # 'Checkin Date' - Raw dates are replaced by numerical features.\n",
    "    # 'WeekDay'      - Replaced by 'WeekDay_num'.\n",
    "    # 'Days'         - Always constant (5) across the dataset.\n",
    "    # 'Snapshot Date' - Constant within a given snapshot.\n",
    "    # 'Snapshot ID'  - Constant within the dataset.\n",
    "    columns_to_drop = ['Hotel Name', 'Snapshot Date', 'Checkin Date', 'WeekDay', 'Days', 'Snapshot ID' ]\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df\n",
    "engineered_data = feature_engineering(relevant_subset.copy())\n",
    "\n",
    "print(\"Engineered Data (first 5 rows):\")\n",
    "display(engineered_data.head())\n",
    "print(engineered_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize and scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are we doing?**  \n",
    "We use a `StandardScaler` to standardize the continuous features by removing the mean and scaling to unit variance. This process prevents features with larger numeric ranges from dominating the learning process.\n",
    "\n",
    "**Why are we doing it?**  \n",
    "Normalization is crucial in many machine learning algorithms—especially when using distance-based metrics or gradient descent—for faster convergence and improved performance.\n",
    "\n",
    "**Features to Scale:**  \n",
    "Based on our engineered dataset, we scale:\n",
    "- `Checkin_Date_num` (numerical representation of the check-in date)  \n",
    "- `Original Price`  \n",
    "- `Discount Price`  \n",
    "- `Available Rooms`  \n",
    "- `DayDiff`  \n",
    "- `DiscountDiff`  \n",
    "- `DiscountPerc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_features(df, features_to_scale, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    df_scaled[features_to_scale] = scaler.fit_transform(df_scaled[features_to_scale])\n",
    "    \n",
    "    return df_scaled, scaler\n",
    "\n",
    "features_to_scale = [\n",
    "    \"Checkin_Date_num\", \n",
    "    \"Original Price\", \n",
    "    \"Discount Price\", \n",
    "    \"Available Rooms\", \n",
    "    \"DayDiff\", \n",
    "    \"DiscountDiff\", \n",
    "    \"DiscountPerc\"\n",
    "]\n",
    "\n",
    "normalized_data, fitted_scaler = normalize_features(engineered_data, features_to_scale)\n",
    "\n",
    "print(\"Normalized Data (first 5 rows):\")\n",
    "display(normalized_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Splitting into X and y & Creating the Wrapper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we:\n",
    "\n",
    "- Split the normalized data into features (`X`) and target (`y`). Here, `y` is the scaled \"Discount Price\".  \n",
    "- Create a wrapper function that runs the entire preprocessing pipeline (from getting relevant data to normalization) and returns `X`, `y`, the fitted scaler, and a helper function to reverse the scaling for the target.  \n",
    "- The helper function uses the scaling parameters for \"Discount Price\" (found in the fitted scaler) so that later we can convert predictions back to their original values.\n",
    "\n",
    "> **Why Remove \"Discount Price\" from X?**  \n",
    "> The target variable should not be present in the features. By dropping \"Discount Price\" from the normalized data, we ensure that `X` contains only the input features while `y` contains the target variable.\n",
    "\n",
    "> **Reversing the Scaling:**  \n",
    "> Since the scaler is fitted on multiple columns, we locate the index corresponding to \"Discount Price\" in our `features_to_scale` list and use its mean and scale to convert scaled predictions back to their original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(params, data, sort=False):\n",
    "    relevant = load_relevant_data(params, data)\n",
    "    engineered = feature_engineering(relevant.copy())\n",
    "    normalized, scaler = normalize_features(engineered, features_to_scale)\n",
    "    price_related_columns = [\"Discount Price\",\"DiscountPerc\", \"DiscountDiff\", \"Original Price\"]\n",
    "    if sorted:\n",
    "        normalized = normalized.sort_values(by=\"Discount Price\")\n",
    "\n",
    "    X = normalized.drop(columns=price_related_columns)\n",
    "    y = normalized[\"Discount Price\"]\n",
    "\n",
    "    idx = features_to_scale.index(\"Discount Price\")\n",
    "    \n",
    "    def reverse_scaling(y_scaled):\n",
    "        return y_scaled * scaler.scale_[idx] + scaler.mean_[idx]\n",
    "    \n",
    "    return X, y, scaler, reverse_scaling\n",
    "\n",
    "X, y, scaler, reverse_scaling = prepare_training_data(test_params,data_filtered )\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"First 5 values of y (scaled):\", y.head().values)\n",
    "print(\"First 5 values of y (original):\", reverse_scaling(y.head().values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression: Training And Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we initialize our GaussianProcessRegressor model. We use a kernel that combines a ConstantKernel and an RBF kernel. \n",
    "\n",
    "- **GaussianProcessRegressor:**  \n",
    "  Provides both predictions and uncertainty estimates, which are essential for our active sampling approach.\n",
    "\n",
    "- **ConstantKernel:**  \n",
    "  Represents a constant bias in the function, serving as a baseline.\n",
    "\n",
    "- **RBF Kernel (Radial Basis Function):**  \n",
    "  Models smooth variations in the data, which is ideal for capturing trends in discount prices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "def initialize_model():\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2, normalize_y=True)\n",
    "    return gp\n",
    "\n",
    "model = initialize_model()\n",
    "print(\"Initialized Model:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Sampling Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum uncertainty based stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we implement the active sampling loop. The process is as follows:\n",
    "\n",
    "- Start with an initial set of samples (e.g., the first, middle, and last indices).\n",
    "- Iteratively fit the Gaussian Process model on the current training set.\n",
    "- Predict on the remaining (unsampled) data and obtain uncertainty estimates.\n",
    "- Select the candidate with the highest uncertainty and add it to the training set.\n",
    "- Compute the `R²` score to monitor performance.\n",
    "- Stop when the maximum uncertainty is below a threshold or when the maximum iterations are reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def active_sampling_loop(X, y, max_iterations=10, uncertainty_threshold=0.05, initial_sample_indices=None):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    if initial_sample_indices is None:\n",
    "        initial_sample_indices = [0, n_samples // 2, n_samples - 1]\n",
    "    \n",
    "    training_indices = set(initial_sample_indices)\n",
    "    candidate_indices = set(range(n_samples)) - training_indices\n",
    "    iteration_log = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Prepare current training data\n",
    "        X_train = X.iloc[sorted(list(training_indices))]\n",
    "        y_train = y.iloc[sorted(list(training_indices))]\n",
    "        \n",
    "        # Initialize and fit the Gaussian Process model\n",
    "        model = initialize_model()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on candidate data and obtain uncertainties\n",
    "        candidate_list = sorted(list(candidate_indices))\n",
    "        if not candidate_list:\n",
    "            break\n",
    "\n",
    "        X_candidates = X.iloc[candidate_list]\n",
    "        y_pred_candidates, std_candidates = model.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        # Find candidate with maximum uncertainty\n",
    "        max_std = np.max(std_candidates)\n",
    "        chosen_candidate_idx = candidate_list[np.argmax(std_candidates)]\n",
    "        \n",
    "        y_pred_full = model.predict(X)\n",
    "        current_r2 = r2_score(y, y_pred_full)\n",
    "        iteration_log.append({\n",
    "            'iteration': iteration,\n",
    "            'num_samples': len(training_indices),\n",
    "            'max_uncertainty': max_std,\n",
    "            'r2': current_r2\n",
    "        })\n",
    "        \n",
    "        \n",
    "        if max_std < uncertainty_threshold:\n",
    "            break\n",
    "        \n",
    "        # Add the most uncertain candidate to the training set\n",
    "        training_indices.add(chosen_candidate_idx)\n",
    "        candidate_indices.remove(chosen_candidate_idx)\n",
    "    \n",
    "    # Final training on the selected samples\n",
    "    X_final = X.iloc[sorted(list(training_indices))]\n",
    "    y_final = y.iloc[sorted(list(training_indices))]\n",
    "    final_model = initialize_model()\n",
    "    final_model.fit(X_final, y_final)\n",
    "    final_predictions = final_model.predict(X)\n",
    "    final_r2 = r2_score(y, final_predictions)\n",
    "    \n",
    "    return {\n",
    "        'final_model': final_model,\n",
    "        'final_predictions': final_predictions,\n",
    "        'final_r2': final_r2,\n",
    "        'iteration_log': iteration_log,\n",
    "        'total_samples_used': len(training_indices)\n",
    "    }\n",
    "\n",
    "results = active_sampling_loop(X, y, max_iterations=15, uncertainty_threshold=0.05)\n",
    "\n",
    "print(\"Final R² Score:\", results['final_r2'])\n",
    "print(\"Total Samples Used:\", results['total_samples_used'])\n",
    "print(\"Iteration Log:\")\n",
    "for log in results['iteration_log']:\n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum uncertainty and stagnant Iterations based stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we implement an enhanced active sampling loop. In addition to stopping when the maximum uncertainty is very low, we also monitor `R²` improvement. The loop will stop if either:\n",
    "- The model's uncertainty is very low (`max_std < uncertainty_threshold`), or\n",
    "- The uncertainty is moderately low (`max_std < higher_uncertainty_threshold`) **and** the `R²` improvement has been stagnant for a set number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling_loop_with_stagnant_iterations_stop(X, y, max_iterations=10, \n",
    "                                                        uncertainty_threshold=0.05, \n",
    "                                                        higher_uncertainty_threshold=0.1,\n",
    "                                                        initial_sample_indices=None, \n",
    "                                                        r2_improvement_threshold=1e-4, \n",
    "                                                        max_stagnant_iterations=3):\n",
    "    n_samples = X.shape[0]\n",
    "    if initial_sample_indices is None:\n",
    "        initial_sample_indices = [0, n_samples // 2, n_samples - 1]\n",
    "    \n",
    "    training_indices = set(initial_sample_indices)\n",
    "    candidate_indices = set(range(n_samples)) - training_indices\n",
    "    iteration_log = []\n",
    "    \n",
    "    previous_r2 = -np.inf  \n",
    "    stagnant_iterations = 0  \n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Prepare training data for the current iteration\n",
    "        X_train = X.iloc[sorted(list(training_indices))]\n",
    "        y_train = y.iloc[sorted(list(training_indices))]\n",
    "        \n",
    "        # Initialize and fit the model\n",
    "        model = initialize_model()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on candidate data to obtain uncertainties\n",
    "        candidate_list = sorted(list(candidate_indices))\n",
    "        if not candidate_list:\n",
    "            break\n",
    "\n",
    "        X_candidates = X.iloc[candidate_list]\n",
    "        y_pred_candidates, std_candidates = model.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        max_std = np.max(std_candidates)\n",
    "        chosen_candidate_idx = candidate_list[np.argmax(std_candidates)]\n",
    "        \n",
    "        \n",
    "        y_pred_full = model.predict(X)\n",
    "        current_r2 = r2_score(y, y_pred_full)\n",
    "        \n",
    "        \n",
    "        r2_improvement = current_r2 - previous_r2\n",
    "        if r2_improvement < r2_improvement_threshold:\n",
    "            stagnant_iterations += 1\n",
    "        else:\n",
    "            stagnant_iterations = 0\n",
    "        \n",
    "        previous_r2 = current_r2  \n",
    "        \n",
    "        iteration_log.append({\n",
    "            'iteration': iteration,\n",
    "            'num_samples': len(training_indices),\n",
    "            'max_uncertainty': max_std,\n",
    "            'r2': current_r2,\n",
    "            'r2_improvement': r2_improvement\n",
    "        })\n",
    "        \n",
    "        \n",
    "        # Stop if the model is very confident OR if it is moderately confident and not improving.\n",
    "        if (max_std < uncertainty_threshold) or ((max_std < higher_uncertainty_threshold) and (stagnant_iterations >= max_stagnant_iterations)):\n",
    "            break\n",
    "        \n",
    "        # Add the candidate with highest uncertainty to the training set\n",
    "        training_indices.add(chosen_candidate_idx)\n",
    "        candidate_indices.remove(chosen_candidate_idx)\n",
    "    \n",
    "    # Final training on the selected samples\n",
    "    X_final = X.iloc[sorted(list(training_indices))]\n",
    "    y_final = y.iloc[sorted(list(training_indices))]\n",
    "    final_model = initialize_model()\n",
    "    final_model.fit(X_final, y_final)\n",
    "    final_predictions = final_model.predict(X)\n",
    "    final_r2 = r2_score(y, final_predictions)\n",
    "    \n",
    "    return {\n",
    "        'final_model': final_model,\n",
    "        'final_predictions': final_predictions,\n",
    "        'final_r2': final_r2,\n",
    "        'iteration_log': iteration_log,\n",
    "        'total_samples_used': len(training_indices)\n",
    "    }\n",
    "\n",
    "\n",
    "results = active_sampling_loop_with_stagnant_iterations_stop(X, y, max_iterations=15, \n",
    "                                                              uncertainty_threshold=0.05, \n",
    "                                                              higher_uncertainty_threshold=0.1,\n",
    "                                                              r2_improvement_threshold=1e-4, \n",
    "                                                              max_stagnant_iterations=3)\n",
    "\n",
    "print(\"Final R² Score:\", results['final_r2'])\n",
    "print(\"Total Samples Used:\", results['total_samples_used'])\n",
    "print(\"Iteration Log:\")\n",
    "for log in results['iteration_log']:\n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell we visualize the active sampling process. The first part prints a log that shows, for each iteration, the number of samples used, the maximum uncertainty, and the R² score. The chart below shows two plots:\n",
    "- **R² Evolution:** How the model's performance improves over iterations.\n",
    "- **Max Uncertainty Evolution:** How the maximum prediction uncertainty decreases as more samples are added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_iteration_log(results, total_data_count):\n",
    "    final_samples = results['total_samples_used']\n",
    "    percent_used = final_samples / total_data_count * 100\n",
    "    print(f\"Total Data Available: {total_data_count}\")\n",
    "    print(f\"Total Samples Used: {final_samples} ({percent_used:.2f}%)\")\n",
    "    print(\"\\nIteration Log:\")\n",
    "    for log in results['iteration_log']:\n",
    "        print(f\"Iteration {log['iteration']}: {log['num_samples']} samples, \"\n",
    "              f\"Max Uncertainty: {log['max_uncertainty']:.4f}, R²: {log['r2']:.4f}\")\n",
    "    print(f\"\\nFinal R² Score: {results['final_r2']:.4f}\")\n",
    "\n",
    "def plot_active_sampling_results(results, total_data_count):\n",
    "    iterations = [log['iteration'] for log in results['iteration_log']]\n",
    "    r2_values = [log['r2'] for log in results['iteration_log']]\n",
    "    uncertainties = [log['max_uncertainty'] for log in results['iteration_log']]\n",
    "    \n",
    "    final_samples = results['total_samples_used']\n",
    "    percent_used = final_samples / total_data_count * 100\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "    \n",
    "    ax[0].plot(iterations, r2_values, marker='o', linestyle='-')\n",
    "    ax[0].set_title('R² Evolution Over Iterations')\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    ax[0].set_ylabel('R² Score')\n",
    "    ax[0].grid(True)\n",
    "    ax[0].text(0.5, 0.1, f'Final R²: {results[\"final_r2\"]:.4f}', \n",
    "               transform=ax[0].transAxes, fontsize=12, color='green',\n",
    "               bbox=dict(facecolor='white', alpha=0.8, edgecolor='green'))\n",
    "    \n",
    "    \n",
    "    ax[1].plot(iterations, uncertainties, marker='o', linestyle='-')\n",
    "    ax[1].set_title('Max Uncertainty Evolution Over Iterations')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Max Uncertainty (std)')\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Active Sampling: {final_samples} of {total_data_count} samples used ({percent_used:.2f}%)', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "total_data_count = X.shape[0]  # For example, if X has 44 rows\n",
    "\n",
    "print_iteration_log(results, total_data_count)\n",
    "plot_active_sampling_results(results, total_data_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression - Full dataset Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Sampling Model Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we execute our active sampling models over all parameter combinations. We provide two variants:\n",
    "- **Basic Active Sampling Loop:** The standard loop that iteratively selects samples based on maximum uncertainty.\n",
    "- **Active Sampling Loop with Stagnant Iterations Stop:** An enhanced loop that also monitors `R²` improvements and stops if the model's performance stagnates while uncertainty is moderately low.\n",
    "\n",
    "This section prepares the results for each parameter combination by running the appropriate active sampling function and collecting key metrics such as the final R², total samples used, and iteration logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_active_sampling_basic(params_list, data, sort=False, verbose=False, **hyperparams):\n",
    "    results_list = []\n",
    "    if not verbose:\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for i, param in enumerate(tqdm(params_list, desc=\"Running basic active sampling\", disable=verbose)):\n",
    "        X, y, scaler, reverse_scaling = prepare_training_data(param, data, sort=sort)\n",
    "        total_data_count = X.shape[0] \n",
    "        \n",
    "        result = active_sampling_loop(X, y, **hyperparams)\n",
    "        result['params'] = param\n",
    "        result['total_data_count'] = total_data_count\n",
    "        results_list.append(result)\n",
    "        if verbose:\n",
    "            print(f\"Completed parameter {i+1}/{len(params_list)}: R² = {result['final_r2']:.4f}, Samples = {result['total_samples_used']} of {X.shape[0]}\")\n",
    "            \n",
    "    warnings.filterwarnings(\"default\", category=ConvergenceWarning)\n",
    "\n",
    "    return results_list\n",
    "\n",
    "basic_hyperparams = {\n",
    "    \"max_iterations\": 15,\n",
    "    \"uncertainty_threshold\": 0.05,\n",
    "}\n",
    "\n",
    "results_basic = run_active_sampling_basic(\n",
    "params_mostData[:3], data_filtered, \n",
    "verbose=True,\n",
    "**basic_hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_active_sampling_stagnant(params_list, data, sort=False ,verbose=False, **hyperparams):\n",
    "    results_list = []\n",
    "    if not verbose:\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for i, param in enumerate(tqdm(params_list, desc=\"Running stagnant active sampling\", disable=verbose)):\n",
    "        X, y, scaler, reverse_scaling = prepare_training_data(param, data, sort=sort)\n",
    "        total_data_count = X.shape[0] \n",
    "        \n",
    "        result = active_sampling_loop_with_stagnant_iterations_stop(X, y, **hyperparams)\n",
    "        result['params'] = param\n",
    "        result['total_data_count'] = total_data_count \n",
    "        results_list.append(result)\n",
    "        if verbose:\n",
    "            print(f\"Completed parameter {i+1}/{len(params_list)}: R² = {result['final_r2']:.4f}, Samples = {result['total_samples_used']} of {total_data_count}\")\n",
    "    \n",
    "    warnings.filterwarnings(\"default\", category=ConvergenceWarning)\n",
    "\n",
    "    return results_list\n",
    "\n",
    "stagnant_hyperparams = {\n",
    "    \"max_iterations\": 15,\n",
    "    \"uncertainty_threshold\": 0.05,\n",
    "    \"higher_uncertainty_threshold\": 0.1,\n",
    "    \"r2_improvement_threshold\": 1e-4,\n",
    "    \"max_stagnant_iterations\": 3\n",
    "}\n",
    "\n",
    "results_stagnant = run_active_sampling_stagnant(\n",
    "params_mostData[:3], data_filtered,\n",
    "verbose =True,\n",
    "**stagnant_hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Visualization & Preliminary Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we visualize the aggregated results. The visualization function now accepts:\n",
    "- `loop_type`: A string indicating the active sampling variant (e.g., \"Basic Active Loop\" or \"Stagnant Active Loop\").\n",
    "- `hyperparams`: A dictionary of hyperparameter values used for the active sampling loop.\n",
    "\n",
    "The function then creates a title that includes these hyperparameter values and displays three bar charts:\n",
    "- Final `R²` scores.\n",
    "- Total samples used.\n",
    "- Data utilization percentages (samples used as a percentage of total available data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_aggregated_results(results_list, loop_type, hyperparams):\n",
    "    title_str = f\"{loop_type} Results\"\n",
    "    \n",
    "    hyperparams_items = [f\"{k}: {v}\" for k, v in hyperparams.items()]\n",
    "    hyperparams_str = \"\"\n",
    "    for i in range(0, len(hyperparams_items), 3):\n",
    "        chunk = \" | \".join(hyperparams_items[i:i+3])\n",
    "        hyperparams_str += chunk + (\"\\n\" if i + 3 < len(hyperparams_items) else \"\")\n",
    "    \n",
    "    final_r2_list = [r['final_r2'] for r in results_list]\n",
    "    samples_used_list = [r['total_samples_used'] for r in results_list]\n",
    "    total_data_counts = [r['total_data_count'] for r in results_list]\n",
    "    data_utilization_list = [ (used / total) * 100 for used, total in zip(samples_used_list, total_data_counts)]\n",
    "    \n",
    "    n = len(results_list)\n",
    "    indices = range(n)\n",
    "    \n",
    "    avg_r2 = sum(final_r2_list) / n\n",
    "    avg_samples = sum(samples_used_list) / n\n",
    "    avg_utilization = sum(data_utilization_list) / n\n",
    "    \n",
    "    # Create subplots for the aggregated metrics.\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Bar chart for final R².\n",
    "    axes[0].bar(indices, final_r2_list, color='skyblue')\n",
    "    axes[0].axhline(avg_r2, color='red', linestyle='--', label=f'Avg R² = {avg_r2:.4f}')\n",
    "    axes[0].set_xlabel('Parameter Combination Index')\n",
    "    axes[0].set_ylabel('Final R²')\n",
    "    axes[0].set_title('Final R² for Each Combination')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Bar chart for total samples used.\n",
    "    axes[1].bar(indices, samples_used_list, color='lightgreen')\n",
    "    axes[1].axhline(avg_samples, color='red', linestyle='--', label=f'Avg Samples = {avg_samples:.2f}')\n",
    "    axes[1].set_xlabel('Parameter Combination Index')\n",
    "    axes[1].set_ylabel('Total Samples Used')\n",
    "    axes[1].set_title('Samples Used per Combination')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Bar chart for data utilization (%).\n",
    "    axes[2].bar(indices, data_utilization_list, color='plum')\n",
    "    axes[2].axhline(avg_utilization, color='red', linestyle='--', label=f'Avg Utilization = {avg_utilization:.2f}%')\n",
    "    axes[2].set_xlabel('Parameter Combination Index')\n",
    "    axes[2].set_ylabel('Data Utilization (%)')\n",
    "    axes[2].set_title('Data Utilization per Combination')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    # Set a prominent super-title at the top.\n",
    "    plt.suptitle(title_str, fontsize=20, fontweight='bold', color='navy', y=0.98)\n",
    "    \n",
    "    # Add a centered hyperparameters container below the title.\n",
    "    # This container is placed at y=0.93 and is horizontally centered (x=0.5).\n",
    "    plt.gcf().text(0.5, 0.85, f\"Hyperparameters:\\n\\n{hyperparams_str}\", \n",
    "                   fontsize=12, ha='center', va='center',\n",
    "                   bbox=dict(facecolor='lightgrey', alpha=0.6, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Adjust the layout so that subplots start below the hyperparameters container.\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.83])\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average Final R²: {avg_r2:.4f}\")\n",
    "    print(f\"Average Samples Used: {avg_samples:.2f}\")\n",
    "    print(f\"Average Data Utilization: {avg_utilization:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_basic = run_active_sampling_basic(params_mostData, data_filtered, verbose=False, **basic_hyperparams)\n",
    "visualize_aggregated_results(results_basic, \"Basic Active Loop\", basic_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stagnant = run_active_sampling_stagnant(params_mostData, data_filtered, verbose=False, **stagnant_hyperparams)\n",
    "visualize_aggregated_results(results_stagnant, \"Stagnant Active Loop\", stagnant_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminary Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "\n",
    "![Basic results updated](./images/step-4-Basic-initial-results.png)\n",
    "\n",
    "![Stagnant results](./images/step-4-stagnat-initial-results.png)  \n",
    "\n",
    "**Basic V.S. Stagnant**\n",
    "\n",
    "- **Basic Active Sampling Loop:**\n",
    "  - **Robustness:** Tends to use around 18 samples per parameter combination, yielding high `R²` scores (often 1.0000).\n",
    "  - **Efficiency:** Uses more data, which may be unnecessary in cases with abundant information.\n",
    "\n",
    "- **Stagnant Active Sampling Loop:**\n",
    "  - **Efficiency:** Stops early when `R²` improvement stagnates, generally using fewer samples (between 11–16) and still achieving high performance in most cases.\n",
    "  - **Risks:** In some cases (e.g., parameter combinations with very few total data points), the loop stops after only 3 samples, leading to negative `R²` values and clear underfitting.\n",
    "\n",
    " **Outliers and Data Issues**\n",
    "\n",
    "- **Outlier Cases:**  \n",
    "  Some parameter combinations consistently use only 3 samples and report poor performance (e.g., negative `R²`). These cases likely represent scenarios where the available data is sparse or noisy, right now its looking like  the most promising direction to investigate and improve.\n",
    "\n",
    "- **Implication:**  \n",
    "  Relying solely on the stagnant loop's stopping criterion can result in premature termination. A minimum sample threshold may be necessary to ensure that the model has enough data to learn meaningful patterns.\n",
    "\n",
    " **Next Steps**\n",
    "\n",
    "1. **Revisit the Data:**  \n",
    "   Analyze the outlier parameter combinations with low total data counts. Determine if these cases should be treated differently or if more data can be acquired.\n",
    "\n",
    "2. **Implement a Minimum Sample Threshold:**  \n",
    "   Adjust the stagnant loop to enforce a minimum number of samples (e.g., at least 5 or 7 samples) before allowing early termination (probably most promising result).\n",
    "\n",
    "3. **Hyperparameter Tuning:**  \n",
    "   Experiment with different settings for the uncertainty thresholds and maximum stagnant iterations to balance efficiency and robustness. Fine-tuning may reduce instances where the model stops too early.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Optimization and Data Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, would try to improve our results and address the points raised in the [preliminary conclusions](#preliminary-conclusions)\n",
    "\n",
    "- Revisit the Data\n",
    "- Implement a Minimum Sample Threshold\n",
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Revisit Parameter Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we re-calculate the parameter combinations using our two strategies:\n",
    "- **onePerHotel:** For each hotel, we select the combination (of `Snapshot Date`, `Hotel Name`, and `Discount Code`) with the highest transaction count.\n",
    "- **mostData:** We select the top 40 overall combinations based on transaction counts.\n",
    "\n",
    "We then display these results in a table (including the transaction counts) so we can review the distribution and identify any potential outliers in data availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_filtered.groupby(\n",
    "    ['Snapshot Date', 'Hotel Name', 'Discount Code']\n",
    ").size().reset_index(name='transaction_count')\n",
    "\n",
    "onePerHotel = (\n",
    "    grouped.sort_values(['Hotel Name', 'transaction_count'], ascending=[True, False])\n",
    "    .groupby('Hotel Name')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "onePerHotel = onePerHotel.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "mostData = grouped.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "print(\"One Per Hotel (Top 40) Parameter Combinations with Transaction Counts:\")\n",
    "display(onePerHotel)\n",
    "\n",
    "print(\"\\nMost Data (Top 40) Parameter Combinations with Transaction Counts:\")\n",
    "display(mostData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks solid to me, shows again the for maximum data we rather stick to Most Data params instead of One Per Hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Investigating Valid Data and Outliers for Parameter Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we further analyze the data returned by `load_relevant_data` for each parameter combination. For each combination, we compute:\n",
    "- Total rows (number of transactions)\n",
    "- Unique counts for WeekDays, Snapshot IDs, Check-in Dates, and Available Rooms\n",
    "- Weekend count and Holiday-Connected count (using US holidays for 2014–2016, with a check for a holiday occurring within 3 days before or 1 day after the check-in date)\n",
    "- Data range (from snapshot date to snapshot date + 30 days)\n",
    "- **Price Metrics:**\n",
    "  - Minimum and maximum Discount Price\n",
    "  - Price range as a percentage\n",
    "  - An array of price points (each price point is `[price, showcount]`)\n",
    "\n",
    "We then split the parameter combinations into two groups (Good Predictions vs. Suspected Outliers) based on the final `R²` value and display an average comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "# Create a US holidays object for the years 2014-2016.\n",
    "us_holidays = holidays.US(years=[2014, 2015, 2016])\n",
    "\n",
    "def is_connected_to_holiday(checkin_date, window_before=3, window_after=1):\n",
    "    checkin_date = pd.to_datetime(checkin_date)\n",
    "    start_window = checkin_date - pd.Timedelta(days=window_before)\n",
    "    end_window = checkin_date + pd.Timedelta(days=window_after)\n",
    "    for holiday_date in us_holidays.keys():\n",
    "        holiday_ts = pd.to_datetime(holiday_date)\n",
    "        if start_window <= holiday_ts <= end_window:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "summary_metrics = []\n",
    "for result in results_stagnant:  # (Use results from the stagnant or basic loop as needed)\n",
    "    param = result['params']\n",
    "    df_subset = load_relevant_data(param, data_filtered)\n",
    "    total_rows = df_subset.shape[0]\n",
    "    \n",
    "    unique_weekdays = df_subset['WeekDay'].nunique() if 'WeekDay' in df_subset.columns else None\n",
    "    unique_snapshot_ids = df_subset['Snapshot ID'].nunique() if 'Snapshot ID' in df_subset.columns else None\n",
    "    unique_checkin_dates = df_subset['Checkin Date'].nunique() if 'Checkin Date' in df_subset.columns else None\n",
    "    unique_rooms = df_subset['Available Rooms'].nunique() if 'Available Rooms' in df_subset.columns else None\n",
    "    \n",
    "    weekend_count = df_subset[df_subset['WeekDay'].isin([\"Friday\", \"Saturday\", \"Sunday\"])].shape[0]\n",
    "    holiday_connected_count = df_subset[df_subset['Checkin Date'].apply(is_connected_to_holiday)].shape[0]\n",
    "    \n",
    "    snapshot_dt = pd.to_datetime(param['Snapshot Date'])\n",
    "    data_range = f\"{snapshot_dt.strftime('%Y-%m-%d')} - {(snapshot_dt + pd.Timedelta(days=30)).strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "    # Price metrics\n",
    "    if total_rows > 0:\n",
    "        discount_price_min = df_subset[\"Discount Price\"].min()\n",
    "        discount_price_max = df_subset[\"Discount Price\"].max()\n",
    "        discount_price_mean = df_subset[\"Discount Price\"].mean()\n",
    "        discount_price_std = df_subset[\"Discount Price\"].std()\n",
    "        price_range_percent = ((discount_price_max - discount_price_min) / discount_price_min) * 100 if discount_price_min != 0 else None\n",
    "        price_counts = df_subset[\"Discount Price\"].value_counts().sort_index()\n",
    "        price_points = [[price, count] for price, count in price_counts.items()]\n",
    "    else:\n",
    "        discount_price_min = discount_price_max = discount_price_mean = discount_price_std = price_range_percent = None\n",
    "        price_points = []\n",
    "    \n",
    "    param_key = f\"{param['Snapshot Date']} | {param['Hotel Name']} | {param['Discount Code']}\"\n",
    "    \n",
    "    summary_metrics.append({\n",
    "        \"Parameter\": param_key,\n",
    "        \"Final R²\": result['final_r2'],\n",
    "        \"Total Rows\": total_rows,\n",
    "        \"Unique WeekDays\": unique_weekdays,\n",
    "        \"Unique Snapshot IDs\": unique_snapshot_ids,\n",
    "        \"Unique Checkin Dates\": unique_checkin_dates,\n",
    "        \"Unique Rooms\": unique_rooms,\n",
    "        \"Weekend Count\": weekend_count,\n",
    "        \"Holiday Connected Count\": holiday_connected_count,\n",
    "        \"Data Range\": data_range,\n",
    "        \"Price Points\": price_points,\n",
    "        \"Price Points length\": len(price_points),\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(summary_metrics)\n",
    "\n",
    "# Split into groups based on final R² (good predictions vs. suspected outliers)\n",
    "df_predicting_well = df_metrics[df_metrics[\"Final R²\"] >= 0]\n",
    "df_outliers = df_metrics[df_metrics[\"Final R²\"] < 0]\n",
    "\n",
    "# Compute averages for numeric columns (price metrics will be averaged only for percentage metrics)\n",
    "avg_good = df_predicting_well.mean(numeric_only=True)\n",
    "avg_bad = df_outliers.mean(numeric_only=True)\n",
    "\n",
    "avg_table = pd.DataFrame({\"Good Predictions\": avg_good, \"Bad Predictions\": avg_bad})\n",
    "print(\"Average Metrics Comparison:\")\n",
    "display(avg_table)\n",
    "\n",
    "print(\"\\nDetailed Metrics for Good Predictions:\")\n",
    "display(df_predicting_well)\n",
    "print(\"\\nDetailed Metrics for Suspected Outliers:\")\n",
    "display(df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " **Average Metrics Comparison**\n",
    "\n",
    "| Metric                      | Good Predictions | Bad Predictions  |\n",
    "|-----------------------------|------------------|------------------|\n",
    "| Final R²                    | 0.999862         | -0.603804        |\n",
    "| Total Rows                  | 27.677419        | 24.888889        |\n",
    "| Unique WeekDays             | 6.419355         | 7.000000         |\n",
    "| Unique Snapshot IDs         | 1.354839         | 1.111111         |\n",
    "| Unique Checkin Dates        | 22.580645        | 23.666667        |\n",
    "| Unique Rooms                | 5.225806         | 9.333333         |\n",
    "| Weekend Count               | 9.741935         | 11.000000        |\n",
    "| Holiday Connected Count     | 2.548387         | 4.111111         |\n",
    "| Price Points Length         | 6.419355         | 2.888889         |\n",
    "\n",
    "*Note: The current dataset used for model training did not include the \"holiday connected\" feature. The holiday connected count is computed solely for this analysis.*\n",
    "\n",
    "**Key Observations**\n",
    "\n",
    "- **Overall Quality:**  \n",
    "  - **Similarities:**  \n",
    "  Both groups have comparable totals for rows, unique weekdays, snapshot IDs, and check-in dates.\n",
    "\n",
    "  - **Differences:**  \n",
    "  The outlier group shows notably higher variability in unique rooms (9.33 vs. 5.23) and a higher holiday connected count (4.11 vs. 2.55).\n",
    "\n",
    "- **Outlier Behavior: Two Subgroups **  \n",
    "  1. **The Carlyle A Rosewood Group:**  \n",
    "   - **Performance:** Final R² values are very low or negative.  \n",
    "   - **Price Data:** Limited price variability (there are only unique 2 price points per data set), with many entries concentrated at one price.  \n",
    "   - **Other Metrics:** Higher unique rooms and holiday connected counts suggest inconsistencies in the data for this hotel.\n",
    "\n",
    "  2. **The New York EDITION and Park Hyatt New York Group:**  \n",
    "    - **Performance:** Despite being flagged as outliers, these cases exhibit high final R² (good model performance).  \n",
    "    - **Price Data:** They display greater diversity in price points and more consistent room data, which might be influenced by market factors not captured in the current model.\n",
    "\n",
    "\n",
    "\n",
    "**Next Steps and Conclusions**\n",
    "\n",
    "- The overall data quality appears acceptable. Although adding the holiday connected feature is an option, my intuition is that it may not significantly improve the model—and could potentially add more noise.\n",
    "- The limited price variability in the Carlyle A Rosewood subgroup is a promising direction for further investigation.\n",
    "- The immediate next step should be to examine the algorithm's stopping criteria. In particular, we need to understand why the active sampling stops after only 3 requests and explore the effects of modifying this behavior to request more data points before termination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we stop? (Minimum Sample Threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "- Why did the model stop sampling after 3 iterations? \n",
    "- what would happen if we set a minimum number of iterations (higher)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging the Stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we investigate why the basic active sampling loop stops after only a few iterations. We focus on three parameter combinations from the suspected outlier group:\n",
    "- **params_mostData[9]** (The New York EDITION)\n",
    "- **params_mostData[21]** (The Carlyle A Rosewood)\n",
    "- **params_mostData[29]** (Park Hyatt New York)\n",
    "\n",
    "We use a debug version of the active sampling loop that prints:\n",
    "- The training and candidate indices at each iteration.\n",
    "- The uncertainty values for each candidate.\n",
    "- The final training Data.\n",
    "- The maximum uncertainty and the chosen candidate index.\n",
    "- The current `R²` score.\n",
    "\n",
    "This information should shed light on why the loop is stopping early, helping us determine if a higher minimum sample threshold might be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling_loop_debug_table(X, y, max_iterations=10, uncertainty_threshold=0.05, initial_sample_indices=None):\n",
    "    n_samples = X.shape[0]\n",
    "    if initial_sample_indices is None:\n",
    "        initial_sample_indices = [0, n_samples // 2, n_samples - 1]\n",
    "    \n",
    "    training_indices = set(initial_sample_indices)\n",
    "    candidate_indices = set(range(n_samples)) - training_indices\n",
    "    iteration_log = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        X_train = X.iloc[sorted(list(training_indices))]\n",
    "        y_train = y.iloc[sorted(list(training_indices))]\n",
    "\n",
    "        model = initialize_model()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        candidate_list = sorted(list(candidate_indices))\n",
    "        X_candidates = X.iloc[candidate_list]\n",
    "        y_pred_candidates, std_candidates = model.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        max_std = np.max(std_candidates)\n",
    "        chosen_candidate_idx = candidate_list[np.argmax(std_candidates)]\n",
    "        \n",
    "        y_pred_full = model.predict(X)\n",
    "        current_r2 = r2_score(y, y_pred_full)\n",
    "        \n",
    "        iteration_details = {\n",
    "            \"Iteration\": iteration,\n",
    "            \"Training Indices\": \", \".join(map(str, sorted(training_indices))),\n",
    "            \"Candidate Indices\": \", \".join(map(str, sorted(candidate_indices))),\n",
    "            \"Candidate Uncertainties\": str(np.round(std_candidates, 4).tolist()),\n",
    "            \"Max Uncertainty\": round(max_std, 4),\n",
    "            \"Chosen Candidate\": chosen_candidate_idx,\n",
    "            \"Current R²\": round(current_r2, 4)\n",
    "        }\n",
    "        iteration_log.append(iteration_details)\n",
    "        \n",
    "        if max_std < uncertainty_threshold:\n",
    "            break\n",
    "        \n",
    "        training_indices.add(chosen_candidate_idx)\n",
    "        candidate_indices.remove(chosen_candidate_idx)\n",
    "    \n",
    "    X_final = X.iloc[sorted(list(training_indices))]\n",
    "    y_final = y.iloc[sorted(list(training_indices))]\n",
    "    final_model = initialize_model()\n",
    "    final_model.fit(X_final, y_final)\n",
    "    final_predictions = final_model.predict(X)\n",
    "    final_r2 = r2_score(y, final_predictions)\n",
    "    \n",
    "    final_details = {\n",
    "        \"Final R²\": round(final_r2, 4),\n",
    "        \"Total Samples Used\": len(training_indices)\n",
    "    }\n",
    "    \n",
    "    print(\"Iteration Details:\")\n",
    "    display(pd.DataFrame(iteration_log))\n",
    "    print(\"\\nFinal Details:\")\n",
    "    display(pd.DataFrame([final_details]))\n",
    "    \n",
    "    print(\"\\nFinal Training Data (first 10 rows):\")\n",
    "    display(X_final.head(10))\n",
    "    print(\"Final Training Target (first 10 rows):\")\n",
    "    display(y_final.head(10))\n",
    "    \n",
    "    return {\n",
    "        'final_model': final_model,\n",
    "        'final_predictions': final_predictions,\n",
    "        'final_r2': final_r2,\n",
    "        'iteration_log': iteration_log,\n",
    "        'total_samples_used': len(training_indices)\n",
    "    }\n",
    "\n",
    "# Debug the basic loop for selected parameters from the suspected outlier group.\n",
    "params_to_debug = [params_mostData[9], params_mostData[21], params_mostData[29]]\n",
    "\n",
    "for param in params_to_debug:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Debugging for parameter: {param['Snapshot Date']} | {param['Hotel Name']} | {param['Discount Code']}\")\n",
    "    X_debug, y_debug, scaler, reverse_scaling = prepare_training_data(param, data_filtered)\n",
    "    result_debug = active_sampling_loop_debug_table(X_debug, y_debug, max_iterations=15, uncertainty_threshold=0.05)\n",
    "    print(\"--------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis and Improvements: Understanding R² and Model Behavior**\n",
    "\n",
    " **What We Found**\n",
    "- **Initial Sample Homogeneity:**  \n",
    "  We observed that the three initial samples (selected using indices `[0, n_samples // 2, n_samples - 1]`) in out outliers cased often turn out to be very similar. having the same discount price. \n",
    "  - This causes the model to quickly learn a prediction near initial indices discount price.\n",
    "\n",
    " **A Reminder About R²**\n",
    "- **R² (Coefficient of Determination):**  \n",
    "  Measures how well the model explains the variance in the target variable.\n",
    "  - **R² ≈ 1:** The model explains nearly all the variability.\n",
    "  - **R² ≈ 0:** The model performs no better than simply predicting the mean.\n",
    "  - **Negative R²:** The model performs worse than predicting the mean.\n",
    "\n",
    " **Illustrative Examples**\n",
    "1. **Example 1:** `[1, 3, 5, 7, 1, 8, 4, 7, 1]`\n",
    "   - **Average Calculation:**  \n",
    "     Sum = 37, Average ≈ 4.11  \n",
    "   - **Scenario:**  \n",
    "     If the model starts with three initial samples that are all `1`, it learns that the target is about 1.  \n",
    "     However, because the overall average is 4.11, predicting 1 is far off, resulting in a very poor model fit (negative R²).\n",
    "\n",
    "2. **Example 2:** `[1, 1, 1, 1, 1, 1, 1, 3, 1]`\n",
    "   - **Average Calculation:**  \n",
    "     Sum = 11, Average ≈ 1.22  \n",
    "   - **Scenario:**  \n",
    "     If the model again starts with three `1`s, it learns that the target is about 1.  \n",
    "     Here, because the overall average is close to 1 (1.22), the prediction is nearly as good as the average, leading to an R² close to 0.\n",
    "\n",
    "These examples illustrate that when initial samples are too similar, the model essentially learns to predict a single value—its prediction is then almost equivalent to the mean, causing R² to be very low or even negative.\n",
    "\n",
    " **How to Improve the Model**\n",
    "**Potential Strategies:**\n",
    "- **Quantile-Based (Sorted) Sampling:**  \n",
    "  - *Pros:* Ensures that the initial samples cover the full range of discount prices.\n",
    "  - *Cons:* Requires additional preprocessing to determine quantile indices. im most cases this would lead to a trade of with sampling amount.\n",
    "- **Increasing the Initial Sample Count:**  \n",
    "  - *Pros:* Captures more variability by starting with more diverse data points.\n",
    "  - *Cons:* Increases computational cost in early iterations + may not solve the problem in all cases (see example 2).\n",
    "- **Random or Stratified Sampling:**  \n",
    "  - *Pros:* Reduces bias and ensures different segments of the data are represented.\n",
    "  - *Cons:* May yield inconsistent results unless a fixed random seed is used + may not solve the problem in all cases (see example 2).\n",
    "\n",
    "**Current Approach:**  \n",
    "We currently sort the sub-dataset by discount price so that the initial samples are drawn from different parts of the distribution. However, when the data are naturally homogeneous, this method may still select very similar points.\n",
    "\n",
    " **Summary**\n",
    "When the initial sample points are too similar, the model essentially learns to predict a value near the average, leading to an R² near 0 or even negative scores. In contrast, a more diverse set of initial samples could improve the model’s ability to capture the data’s variability. Our next steps will focus on refining the sampling strategy—possibly using quantile-based or stratified sampling—and exploring a higher minimum sample threshold to achieve better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the data and visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add the sort option to `prepare_training_data`, if sort=true the data will be sorted by discount price.\n",
    "```py\n",
    "\n",
    "if sorted:\n",
    "        normalized = normalized.sort_values(by=\"Discount Price\")\n",
    "\n",
    "```\n",
    "\n",
    "and added the same option to `run_active_sampling_stagnant` and `run_active_sampling_basic`.\n",
    "\n",
    "lets see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_basic = run_active_sampling_basic(params_mostData, data_filtered, sort=True, verbose=False, **basic_hyperparams)\n",
    "visualize_aggregated_results(results_basic, \"Basic - sorted\", basic_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stagnant = run_active_sampling_stagnant(params_mostData, data_filtered, sort=True ,verbose=False, **stagnant_hyperparams)\n",
    "visualize_aggregated_results(results_stagnant, \"Stagnant - sorted\", stagnant_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic results](./images/step4-basic-sorted-Result(pre-tuning).png)\n",
    "![stagnat results](./images/step4-stagnat-sorted-Result(pre-tuning).png)\n",
    "\n",
    "**Wow!** This improvement is insane! but the data utilization is not so great, 67% percent for basic and 48% for stagnant, we can do better!\n",
    "Lets fine tune the hyper parameters to lose some performance for less trining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning hyperparameters  to get the best R^2 to Data utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to strike the right balance between model performance (as measured by R²) and data efficiency (minimizing the percentage of available data used).\n",
    "In our active sampling loops, the stopping criteria play a key role in this trade-off.\n",
    "\n",
    " **What We’re Tuning**\n",
    "\n",
    "**For the Basic Active Sampling Loop:**\n",
    "- **max_iterations:** The maximum number of iterations allowed.\n",
    "- **uncertainty_threshold:** The threshold below which the model’s uncertainty is considered low enough to stop sampling.\n",
    "\n",
    "**For the Stagnant Active Sampling Loop:**\n",
    "- **max_iterations:** Same as above.\n",
    "- **uncertainty_threshold:** Primary stopping threshold.\n",
    "- **higher_uncertainty_threshold:** A secondary threshold used when combined with stagnation.\n",
    "- **r2_improvement_threshold:** The minimum improvement in R² required between iterations.\n",
    "- **max_stagnant_iterations:** The maximum number of consecutive iterations with minimal R² improvement before stopping.\n",
    "\n",
    "**How would we do that?**\n",
    "\n",
    "We would use **BayesianOptimization** to fine-tune our hyperparameters because it efficiently explores the hyperparameter space by balancing exploration and exploitation. This method requires fewer iterations compared to exhaustive grid search and quickly converges to the best set of parameters that maximize our model's R².\n",
    "\n",
    "Our objective function returns the final R², and we define bounds for key hyperparameters (like uncertainty thresholds, R² improvement thresholds, and the maximum stagnant iterations). The optimized parameters help improve model performance and data utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning with Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-Tuning Hyperparameters with Bayesian Optimization**\n",
    "\n",
    "We use Bayesian Optimization to search for the best hyperparameters for our stagnant active sampling loop. Our objective function, `objective_hyperparams_stagnant/basic`, randomly selects 5 parameter combinations from `params_mostData` (with sorted data) and runs the stagnant active sampling loop on each. It then computes the average final R² and average data utilization. A penalty is applied if the average R² falls below 0.96, ensuring that only hyperparameters yielding strong performance are favored.\n",
    "\n",
    "The optimizer then finds the hyperparameter values that maximize our objective. We save the optimized hyperparameters in a variable (`stagnant/basic_optimized_hyperparameters`) and also pickle them to disk for future use. Finally, we run the stagnant active sampling loop with these optimized parameters and visualize the results.\n",
    "\n",
    "This process ensures we balance high R² performance with low data utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pickle\n",
    "\n",
    "def objective_hyperparams_basic(uncertainty_threshold, max_iterations):\n",
    "    max_iterations = int(max_iterations)\n",
    "    hyperparams = {\"max_iterations\": max_iterations, \"uncertainty_threshold\": uncertainty_threshold}\n",
    "\n",
    "    selected_params = random.sample(params_mostData, 3)\n",
    "    results_list = run_active_sampling_basic(selected_params, data_filtered, sort=True, verbose=False, **hyperparams)\n",
    "    \n",
    "    avg_r2 = sum(result['final_r2'] for result in results_list) / len(results_list)\n",
    "    utilization_vals = [result['total_samples_used'] / result['total_data_count'] for result in results_list]\n",
    "    avg_utilization = sum(utilization_vals) / len(utilization_vals)\n",
    "    score = 1 - avg_utilization\n",
    "    if avg_r2 < 0.96:\n",
    "        penalty = (0.96 - avg_r2) * 100\n",
    "        score -= penalty\n",
    "    return score\n",
    "\n",
    "pbounds_basic = {\"uncertainty_threshold\": (0.01, 0.1), \"max_iterations\": (10, 30)}\n",
    "\n",
    "optimizer_basic = BayesianOptimization(f=objective_hyperparams_basic, pbounds=pbounds_basic, random_state=42)\n",
    "optimizer_basic.maximize(init_points=5, n_iter=150)\n",
    "\n",
    "basic_optimized_hyperparameters = {k: int(v) if k == \"max_iterations\" else v for k, v in optimizer_basic.max['params'].items()}\n",
    "\n",
    "with open(\"basic_optimized_hyperparameters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(basic_optimized_hyperparameters, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"basic_optimized_hyperparameters.pkl\", \"rb\") as f:\n",
    "    basic_optimized_hyperparameters = pickle.load(f)\n",
    "\n",
    "results_basic = run_active_sampling_basic(params_mostData, data_filtered, sort=True, verbose=False, **basic_optimized_hyperparameters)\n",
    "visualize_aggregated_results(results_basic, \"Basic - sorted - optimized \", basic_optimized_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stagnant hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pickle\n",
    "\n",
    "def objective_hyperparams_stagnant(uncertainty_threshold, higher_uncertainty_threshold, r2_improvement_threshold, max_stagnant_iterations, max_iterations):\n",
    "    max_stagnant_iterations = int(max_stagnant_iterations)\n",
    "    max_iterations = int(max_iterations)\n",
    "    hyperparams = {\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"uncertainty_threshold\": uncertainty_threshold,\n",
    "        \"higher_uncertainty_threshold\": higher_uncertainty_threshold,\n",
    "        \"r2_improvement_threshold\": r2_improvement_threshold,\n",
    "        \"max_stagnant_iterations\": max_stagnant_iterations,\n",
    "    }\n",
    "    selected_params = random.sample(params_mostData, 3)\n",
    "    results_list = run_active_sampling_stagnant(selected_params, data_filtered, sort=True, verbose=False, **hyperparams)\n",
    "    avg_r2 = sum(result['final_r2'] for result in results_list) / len(results_list)\n",
    "    utilization_vals = [result['total_samples_used'] / result['total_data_count'] for result in results_list]\n",
    "    avg_utilization = sum(utilization_vals) / len(utilization_vals)\n",
    "    score = 1 - avg_utilization\n",
    "    if avg_r2 < 0.96:\n",
    "        penalty = (0.96 - avg_r2) * 100\n",
    "        score -= penalty\n",
    "    return score\n",
    "\n",
    "pbounds = {\n",
    "    \"uncertainty_threshold\": (0.01, 0.1),\n",
    "    \"higher_uncertainty_threshold\": (0.05, 0.2),\n",
    "    \"r2_improvement_threshold\": (1e-5, 1e-3),\n",
    "    \"max_stagnant_iterations\": (2, 5),\n",
    "    \"max_iterations\": (10, 30),\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_hyperparams_stagnant,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "optimizer.maximize(init_points=5, n_iter=150)\n",
    "\n",
    "stagnant_optimized_hyperparameters = {\n",
    "    k: int(v) if k in ['max_iterations', 'max_stagnant_iterations'] else v \n",
    "    for k, v in optimizer.max['params'].items()\n",
    "}\n",
    "\n",
    "with open(\"stagnant_optimized_hyperparameters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stagnant_optimized_hyperparameters, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"stagnant_optimized_hyperparameters.pkl\", \"rb\") as f:\n",
    "    stagnant_optimized_hyperparameters = pickle.load(f)\n",
    "\n",
    "results_stagnant = run_active_sampling_stagnant(params_mostData, data_filtered, sort=True ,verbose=False, **stagnant_optimized_hyperparameters)\n",
    "visualize_aggregated_results(results_stagnant, \"Stagnant - sorted - optimized\", stagnant_optimized_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters Fine-Tuning Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fine-tuning our hyperparameters using Bayesian Optimization, we achieved impressive improvements in data utilization without sacrificing model performance.\n",
    "\n",
    "**Results visualized**\n",
    "\n",
    "**Before optimization:**\n",
    "![Basic results pre](./images/step4-basic-sorted-Result(pre-tuning).png)\n",
    "![stagnat results pre](./images/step4-stagnat-sorted-Result(pre-tuning).png)\n",
    "\n",
    "\n",
    "**After optimization:**\n",
    "![Basic results post](./images/step4-optimized-basic.png)\n",
    "![stagnat results post](./images/step4-optimized-stagnat.png)\n",
    "\n",
    "\n",
    "**Compression**\n",
    "\n",
    "| **Model**    | **Metric**           | **Before Tuning**               | **After Tuning**                | **Percentage Change**        |\n",
    "|--------------|----------------------|---------------------------------|---------------------------------|------------------------------|\n",
    "| **Stagnant** | Data Utilization     | 48.24% (~12.78 samples/run)     | 37.52% (~9.95 samples/run)         | ~25% reduction               |\n",
    "|              | R² Reduction         | 0% drop                         | ~0.13% drop (<0.0015 reduction)   | ~0.13% drop (negligible)     |\n",
    "| **Basic**    | Data Utilization     | 67.82% (~18 samples/run)      | 42.94% (~11.35 samples/run)         | ~36% reduction               |\n",
    "|              | R² Reduction         | 0% drop                         | No visible drop                | 0% change                   |\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "We are very pleased with the final results. With the tuned hyperparameters, our stagnant model now uses only about 10 samples per run to predict the entire month’s prices with nearly 100% accuracy. This level of efficiency and performance is truly remarkable and demonstrates the strength of our approach in balancing model accuracy with data efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-101-Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
