{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Adding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# !gdown 13HQpkd3ZExNBHiGTi4PVJJ1dn3SKV2MI\n",
    "# hotels_file_path = \"/content/hotels_data.csv\"  \n",
    "hotels_file_path = \"./hotels_data.csv\"  \n",
    "df = pd.read_csv(hotels_file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add columns and load to new CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure dates are in correct format\n",
    "df['Snapshot Date'] = pd.to_datetime(df['Snapshot Date'])\n",
    "df['Checkin Date'] = pd.to_datetime(df['Checkin Date'])\n",
    "\n",
    "df['DayDiff'] = (df['Checkin Date'] - df['Snapshot Date']).dt.days\n",
    "df['WeekDay'] = df['Checkin Date'].dt.day_name()\n",
    "df['DiscountDiff'] = df['Original Price'] - df['Discount Price']\n",
    "df['DiscountPerc'] = (df['DiscountDiff'] / df['Original Price']) * 100\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**New csv format**\n",
    "\n",
    "| Column Name       | Description                                                                                         | Example Value                       |\n",
    "|--------------------|-----------------------------------------------------------------------------------------------------|-------------------------------------|\n",
    "| **Snapshot ID**    | Unique identifier for each snapshot of data                                                        | 1                                   |\n",
    "| **Snapshot Date**  | The date when the snapshot was taken                                                               | 2015-07-17                          |\n",
    "| **Checkin Date**   | The date of check-in for the hotel                                                                 | 2015-08-12                          |\n",
    "| **Days**           | Duration of the stay in days                                                                       | 5                                   |\n",
    "| **Original Price** | Price of the stay without any discount (in dollars)                                                | 1178                                |\n",
    "| **Discount Price** | Price of the stay after applying the discount (in dollars)                                         | 1040                                |\n",
    "| **Discount Code**  | Code representing the type of discount applied (values 1-4, with 1 indicating no discount possible) | 1                                   |\n",
    "| **Available Rooms**| Number of rooms available at the specified check-in date                                           | 6                                   |\n",
    "| **Hotel Name**     | Name of the hotel                                                                                  | Best Western Plus Seaport Inn Downtown |\n",
    "| **Hotel Stars**    | Star rating of the hotel                                                                           | 3                                   |\n",
    "| **DayDiff**        | Number of days between the Snapshot Date and Checkin Date                                          | 26                                  |\n",
    "| **WeekDay**        | Day of the week corresponding to the Checkin Date                                                  | Wednesday                           |\n",
    "| **DiscountDiff**   | Difference between the Original Price and Discount Price (in dollars)                              | 138                                 |\n",
    "| **DiscountPerc**   | Percentage of discount applied, calculated as `(DiscountDiff / Original Price) * 100`             | 11.714770797962649                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_hotels_path = \"./hotels_data_changed.csv\"\n",
    "#df.to_csv(changed_hotels_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Shared) -  Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def convert_weekday_to_num(weekday):\n",
    "    weekday_mapping = {\n",
    "        'Monday': 0,\n",
    "        'Tuesday': 1,\n",
    "        'Wednesday': 2,\n",
    "        'Thursday': 3,\n",
    "        'Friday': 4,\n",
    "        'Saturday': 5,\n",
    "        'Sunday': 6\n",
    "    }\n",
    "    return weekday_mapping.get(weekday, np.nan)\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Process the raw hotel data DataFrame by converting dates, recalculating date differences,\n",
    "    and mapping the weekday to numeric.\n",
    "    \n",
    "    This function adds:\n",
    "      - 'Snapshot_Date_num': Numeric representation of 'Snapshot Date'\n",
    "      - 'Checkin_Date_num' : Numeric representation of 'Checkin Date'\n",
    "      - 'DayDiff'          : Recalculated difference in days between check-in and snapshot\n",
    "      - 'WeekDay_num'      : Numeric representation of the 'WeekDay'\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with additional engineered features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert date strings to datetime objects\n",
    "    df['Snapshot Date'] = pd.to_datetime(df['Snapshot Date'])\n",
    "    df['Checkin Date'] = pd.to_datetime(df['Checkin Date'])\n",
    "    \n",
    "    # Create new numeric date columns using ordinal representation\n",
    "    df['Snapshot_Date_num'] = df['Snapshot Date'].apply(lambda x: x.toordinal())\n",
    "    df['Checkin_Date_num'] = df['Checkin Date'].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    df['DayDiff'] = (df['Checkin Date'] - df['Snapshot Date']).dt.days\n",
    "    \n",
    "    df['WeekDay_num'] = df['WeekDay'].apply(convert_weekday_to_num)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def encode_categorical_columns(df, categorical_cols):\n",
    "    \"\"\"\n",
    "    One-hot encode specified categorical columns.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        categorical_cols (list of str): List of columns to one-hot encode.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with categorical columns encoded.\n",
    "    \"\"\"\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "\n",
    "def normalize_features(df, features_to_scale, scaler=None):\n",
    "    \"\"\"\n",
    "    Normalize specified features in the DataFrame using a given scaler.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input data frame.\n",
    "        features_to_scale (list): List of column names to normalize.\n",
    "        scaler (sklearn.preprocessing object, optional): A scaler instance. \n",
    "            If None, StandardScaler is used.\n",
    "    \n",
    "    Returns:\n",
    "        df_norm (pd.DataFrame): DataFrame with normalized features.\n",
    "        scaler (sklearn.preprocessing object): The fitted scaler.\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "    \n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    df_norm[features_to_scale] = scaler.fit_transform(df_norm[features_to_scale])\n",
    "    \n",
    "    return df_norm, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Best Discount Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A) Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "\n",
    "df = pd.read_csv(changed_hotels_path)\n",
    "original_df = df\n",
    "df = df[['WeekDay', 'Snapshot Date', 'Checkin Date', 'DayDiff', 'Hotel Name', 'Discount Code']] \n",
    "df = df.rename(columns={'Discount Code': 'Class'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "def map_days_to_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    weekday_mapping = {'Sunday': 1, 'Monday': 2, 'Tuesday': 3, 'Wednesday': 4, 'Thursday': 5, 'Friday': 6, 'Saturday': 7}\n",
    "    df['WeekDay'] = df['WeekDay'].map(weekday_mapping)\n",
    "    return df\n",
    "\n",
    "def map_hotel_names_to_numbers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    hotel_mapping = {hotel: i for i, hotel in enumerate(df['Hotel Name'].unique())}\n",
    "    df['Hotel_Index'] = df['Hotel Name'].map(hotel_mapping)\n",
    "    df.drop(['Hotel Name'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def map_date_to_numbers(df: pd.DataFrame, old_coloumn: str, new_coloumn: str) -> pd.DataFrame:\n",
    "    df[old_coloumn] = pd.to_datetime(df[old_coloumn])\n",
    "    df[f'{new_coloumn}_Year'] = df[old_coloumn].dt.year\n",
    "    df[f'{new_coloumn}_Month'] = df[old_coloumn].dt.month\n",
    "    df[f'{new_coloumn}_Day'] = df[old_coloumn].dt.day\n",
    "    df.drop([old_coloumn], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = map_days_to_numbers(df)\n",
    "df = map_hotel_names_to_numbers(df)\n",
    "df = map_date_to_numbers(df, 'Snapshot Date', 'Snapshot')\n",
    "df = map_date_to_numbers(df, 'Checkin Date', 'Checkin')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_features_distribution(df: pd.DataFrame) -> None:\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(12, 15))\n",
    "    fig.suptitle('Distribution of Features')\n",
    "\n",
    "    for i, column in enumerate(df.columns):\n",
    "        row = i // 3  # Calculate row index\n",
    "        col = i % 3    # Calculate column index\n",
    "    \n",
    "        sns.histplot(df[column], bins=10, ax=axes[row, col])\n",
    "        axes[row, col].set_title(f'Distribution of {column}')\n",
    "        axes[row, col].set_xlabel(column)\n",
    "        axes[row, col].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_features_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Irrelevant Features\n",
    "# We see that 2016 doesnt have many values and it can manipulte the data.\n",
    "\n",
    "df = df[df['Checkin_Year'] != 2016]\n",
    "df = df.drop(columns=['Checkin_Year'])\n",
    "\n",
    "df = df[df['Snapshot_Year'] != 2016]\n",
    "df = df.drop(columns=['Snapshot_Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class imbalance\n",
    "\n",
    "class_counts = df['Class'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from itertools import cycle\n",
    "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_categrical_feilds(df):\n",
    "  df['WeekDay'] = df['WeekDay'].astype('category')\n",
    "  df['Hotel_Index'] = df['Hotel_Index'].astype('category')\n",
    "  df['Snapshot_Month'] = df['Snapshot_Month'].astype('category')\n",
    "  df['Checkin_Month'] = df['Checkin_Month'].astype('category')\n",
    "  df['DayDiff'] = df['DayDiff'].astype('category')\n",
    "  df['Snapshot_Day'] = df['Snapshot_Day'].astype('category')\n",
    "  df['Checkin_Day'] = df['Checkin_Day'].astype('category')\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def get_x_y_from_csv():\n",
    "  hotels_predict_discount_path = \"./hotels_predict_discount.csv\"\n",
    "  #hotels_predict_discount_path = \"/content/hotels_predict_discount.csv\"\n",
    "\n",
    "  df = pd.read_csv(hotels_predict_discount_path)\n",
    "  df = change_categrical_feilds(df)\n",
    "\n",
    "  X = df[['WeekDay', 'DayDiff', 'Hotel_Index', 'Snapshot_Month', 'Snapshot_Day', 'Checkin_Month', 'Checkin_Day']]\n",
    "  y = df['Class']\n",
    "\n",
    "  return X, y\n",
    "\n",
    "\n",
    "def get_train_test():\n",
    "  X, y = get_x_y_from_csv()\n",
    "  # XGBost expects [0,1,2,3] instead of [1,2,3,4]\n",
    "  y = y - 1\n",
    "\n",
    "  return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def get_smote_train_test():\n",
    "  X, y = get_x_y_from_csv()\n",
    "  smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "  X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "  return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "def get_undersample_train_test():\n",
    "  X, y = get_x_y_from_csv()\n",
    "  rus = RandomUnderSampler(random_state=42)\n",
    "  X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "  return train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    print_accuracy(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "\n",
    "def print_accuracy(y_train, y_train_pred, y_test, y_test_pred):\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"  Train Accuracy: {train_accuracy:.2f}\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_statistics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=[f'Actual {i+1}' for i in range(cm.shape[0])],\n",
    "                     columns=[f'Predicted {i+1}' for i in range(cm.shape[1])])\n",
    "    print(cm_df)\n",
    "\n",
    "    n_classes = cm.shape[0]\n",
    "\n",
    "    TP = np.diag(cm)  # True Positives per class\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  # False Positives per class\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)  # False Negatives per class\n",
    "\n",
    "    TN = np.zeros(n_classes)\n",
    "    for i in range(n_classes):\n",
    "        # TN for class i: sum of all elements excluding row i and column i\n",
    "        TN[i] = cm.sum() - (cm[i, :].sum() + cm[:, i].sum()) + cm[i, i]\n",
    "\n",
    "    # Calculate TPR (True Positive Rate) and FPR (False Positive Rate) per class\n",
    "    TPR = TP / (TP + FN)  # Sensitivity or Recall\n",
    "    FPR = FP / (FP + TN)\n",
    "\n",
    "    # Handle division by zero\n",
    "    TPR = np.where((TP + FN) == 0, 0, TPR)\n",
    "    FPR = np.where((FP + TN) == 0, 0, FPR)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nMetrics per Class:\")\n",
    "    for i in range(n_classes):\n",
    "        print(f\"Class {i+1}: True Positive Rate (TPR): {TPR[i]:.4f}, False Positive Rate (FPR): {FPR[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_counts(model, X_test, y_test, y_train):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_train_counts = pd.Series(y_train).value_counts()\n",
    "    y_test_counts = pd.Series(y_test).value_counts()\n",
    "    unique_classes, y_pred_counts = np.unique(y_pred, return_counts=True)\n",
    "    y_pred_counts = pd.Series(y_pred_counts, index=unique_classes)\n",
    "\n",
    "    all_classes = sorted(list(set(y_train.unique()).union(set(y_test.unique())).union(set(y_pred))))\n",
    "\n",
    "    data = {\n",
    "        'Class': all_classes,\n",
    "        'y_train_counts': [y_train_counts.get(c, 0) for c in all_classes],\n",
    "        'y_test_counts': [y_test_counts.get(c, 0) for c in all_classes],\n",
    "        'y_pred_counts': [y_pred_counts.get(c, 0) for c in all_classes]\n",
    "    }\n",
    "\n",
    "    df_counts = pd.DataFrame(data)\n",
    "    df_counts['Class'] = df_counts['Class'] + 1\n",
    "\n",
    "    display(df_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate__model_extended(model, X_test, y_test, model_name):\n",
    "    n_classes = 4\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Binarize the output for ROC\n",
    "    y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "    \n",
    "    # Initialize dictionaries for fpr, tpr, and roc_auc\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Predict labels for other metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate per-class FPR and FNR\n",
    "    fpr_per_class = []\n",
    "    fnr_per_class = []\n",
    "    for i in range(n_classes):\n",
    "        tn = np.sum(cm[:, :i].sum() + cm[:, i+1:].sum())  # True Negatives for class i\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]  # False Positives for class i\n",
    "        fn = cm[i, :].sum() - cm[i, i]  # False Negatives for class i\n",
    "        tp = cm[i, i]  # True Positives for class i\n",
    "        fpr_per_class.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "        fnr_per_class.append(fn / (fn + tp) if (fn + tp) > 0 else 0)\n",
    "    \n",
    "    # Compute global metrics (macro average)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMetrics for {model_name}:\")\n",
    "    print(f\"ROC AUC (Micro): {roc_auc['micro']:.4f}\")\n",
    "    print(f\"Per-class ROC AUC: {[f'{i}: {roc_auc[i]:.4f}' for i in range(n_classes)]}\")\n",
    "    print(f\"Per-class FPR: {[f'{i}: {fpr_per_class[i]:.4f}' for i in range(n_classes)]}\")\n",
    "    print(f\"Per-class FNR: {[f'{i}: {fnr_per_class[i]:.4f}' for i in range(n_classes)]}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "    \n",
    "    return fpr, tpr, roc_auc, y_pred, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Printing Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_naive_bayes_params(nb):\n",
    "    params = nb.get_params()\n",
    "\n",
    "    print(f\"  Var smoothing: {params['var_smoothing']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decision_tree_params(dt):\n",
    "    params = dt.get_params()\n",
    "\n",
    "    print(f\"  Criterion: {params['criterion']}\")\n",
    "    print(f\"  Max Depth: {params['max_depth']}\")\n",
    "    print(f\"  Depth: {dt.get_depth()}\")\n",
    "    print(f\"  Num Leaves: {dt.get_n_leaves()}\")\n",
    "    print(f\"  Min Samples Split: {params['min_samples_split']}\")\n",
    "    print(f\"  Min Samples Leaf: {params['min_samples_leaf']}\")\n",
    "    print(f\"  Max Features: {params['max_features']}\")\n",
    "    print(f\"  Max Leaf Nodes: {params['max_leaf_nodes']}\")\n",
    "    print(f\"  Min Impurity Decrease: {params['min_impurity_decrease']}\")\n",
    "    print(f\"  CCP Alpha: {params['ccp_alpha']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_random_forest_params(rf_classifier):\n",
    "    params = rf_classifier.get_params()\n",
    "\n",
    "    print(f\"  N Estimators: {params['n_estimators']}\")\n",
    "    print(f\"  Number of Leaves: {sum(tree.get_n_leaves() for tree in rf_classifier.estimators_)}\")\n",
    "    print(f\"  Criterion: {params['criterion']}\")\n",
    "    print(f\"  Max Depth: {params['max_depth']}\")\n",
    "    print(f\"  Max Features: {params['max_features']}\")\n",
    "    print(f\"  Max Leaf Nodes: {params['max_leaf_nodes']}\")\n",
    "    print(f\"  Max Samples: {params['max_samples']}\")\n",
    "    print(f\"  Min Impurity Decrease: {params['min_impurity_decrease']}\")\n",
    "    print(f\"  Min Samples Leaf: {params['min_samples_leaf']}\")\n",
    "    print(f\"  Min Samples Split: {params['min_samples_split']}\")\n",
    "    print(f\"  Min Weight Fraction Leaf: {params['min_weight_fraction_leaf']}\")\n",
    "    print(f\"  Parameters: {params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_xgb_params(xgb):\n",
    "  booster = xgb.get_booster()\n",
    "\n",
    "  config_str = booster.save_config()\n",
    "  config_dict = json.loads(config_str)\n",
    "\n",
    "  learner_params = config_dict['learner']\n",
    "\n",
    "  print(f\" max_depth: {learner_params['gradient_booster']['tree_train_param']['max_depth']}\")\n",
    "  print(f\" learning_rate: {learner_params['gradient_booster']['tree_train_param']['learning_rate']:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B) Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and evaluate the following five algorithms: Random Forest, Decision Tree, Naïve Bayes, XGBoost, and a simple Random classifier.\n",
    "For each algorithm, you need to experiment with different parameter settings to find the optimal combination that yields the best performance. \n",
    "Explanation of how each chosen parameter affects the algorithm's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "nb_categorial = CategoricalNB()\n",
    "nb_categorial.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(nb_categorial, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_class_counts(nb_categorial, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_statistics(nb_categorial, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparaeters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last model (with default prior=true) gave class 4 little percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_smote_train_test()\n",
    "nb_categorial_smote = CategoricalNB(fit_prior=False)\n",
    "nb_categorial.fit(X_train, y_train)\n",
    "\n",
    "model_statistics(nb_categorial, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no class imbalance but less accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.naive_bayes.CategoricalNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None, min_categories=None)\n",
    "\n",
    "grid_search for alpha(var_smoothing) wasnt better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42) \n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(dt_classifier, X_train, X_test, y_train, y_test)\n",
    "print_decision_tree_params(dt_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HyperParameter Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0, monotonic_cst=None)[source]\n",
    "\n",
    "**criterion: ['gini', 'entropy']** Gini and Entropy are two different ways to calculate the information gain. They can result in different tree structures.\n",
    "\n",
    "splitter - default evaluates all features and chooses the split that maximizes the chosen criterion (\"gini\" or \"entropy\").\n",
    "\n",
    "**max_depth: [3, 10, 20, 30]** last tree was 44 which is too much for 7 features..\n",
    "\n",
    "**min_samples_split: [2, 5, 10]** Increasing min_samples_split forces the tree to make splits only when there are enough samples, preventing it from creating overly specific branches that might fit noise in the training data.\n",
    "\n",
    "**min_samples_leaf: [1, 2, 5]** increasing min_samples_leaf forces the tree to create larger, more generalized leaf nodes, which can improve performance on unseen data.\n",
    "\n",
    "min_weight_fraction_leaf -  minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.\n",
    "\n",
    "**max_features: [None, 5, 3]** None = all. log(7) or sqrt(7) will be 3, and i chose 1 in between. Limiting max_features can sometimes improve performance by reducing overfitting\n",
    "\n",
    "**max_leaf_nodes': [None, 2000, 3500, 5000, 6500]** tree had 56,000 leaf noedes which was too much.\n",
    "\n",
    "**min_impurity_decrease': [0.0, 0.1, 0.2]** sets a minimum threshold for the impurity reduction. A split will only occur if the impurity reduction is greater than or equal to this threshold. It helps prevent the tree from making splits that result in very small impurity reductions, which might indicate overfitting.  \n",
    "\n",
    "class_weight=None\n",
    "\n",
    "**ccp_alpha': [0.0, 0.1, 0.2]** cost-complexity pruning in decision trees.\n",
    "It controls the trade-off between the size of the tree and its accuracy on the training data.   \n",
    "Higher ccp_alpha values lead to more pruning, resulting in smaller trees\n",
    "<img src=\"images/ccp_alpha.png\" width=400 height=300>\n",
    "\n",
    "monotonic_cst=None Indicates the monotonicity constraint to enforce on each feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': [None, 5, 3],\n",
    "    'max_leaf_nodes': [None, 2000, 3500, 5000, 6500],\n",
    "    'min_impurity_decrease': [0.0, 0.1, 0.2],\n",
    "    'ccp_alpha': [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid, cv=5)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "dt_best_model = grid_search.best_estimator_\n",
    "\n",
    "evaluate_model(dt_best_model, X_train, X_test, y_train, y_test)\n",
    "print_decision_tree_params(dt_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "156 minutes vs 16 seconds same accuracy.\n",
    "only needed to add max leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "param_grid = {\n",
    "    'max_leaf_nodes': [None, 2000, 3500, 5000, 6500],\n",
    "}\n",
    "\n",
    "grid_search_2 = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid, cv=5)\n",
    "\n",
    "grid_search_2.fit(X_train, y_train)\n",
    "\n",
    "dt_best_model_2 = grid_search_2.best_estimator_\n",
    "\n",
    "evaluate_model(dt_best_model_2, X_train, X_test, y_train, y_test)\n",
    "print_decision_tree_params(dt_best_model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))  # Adjust figure size as needed\n",
    "plot_tree(dt_classifier, filled=True, feature_names=[f\"feature_{i}\" for i in range(X_train.shape[1])], class_names=[str(c) for c in dt_classifier.classes_]) # generate feature names, and class names\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "rf_classifier = RandomForestClassifier() \n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(rf_classifier, X_train, X_test, y_train, y_test)\n",
    "print_random_forest_params(rf_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HyperParameter Tuninig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "     'max_depth': [None, 3, 7, 15],  # Maximum depth of the tree\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42) #Setting a random state for reproducibility\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "rf_best_model = grid_search.best_estimator_\n",
    "evaluate_model(rf_best_model, X_train, X_test, y_train, y_test)\n",
    "print_random_forest_params(rf_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_xgboost_train_test_semi_categorical()\n",
    "\n",
    "xgb_model = XGBClassifier(objective='multi:softprob', num_class=4, random_state=42, enable_categorical=True) # multi class returns propabilites\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(xgb_model, X_train, X_test, y_train, y_test)\n",
    "print_xgb_params(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with everything categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_xgboost_train_test_all_categorical()\n",
    "\n",
    "xgb_model = XGBClassifier(objective='multi:softprob', num_class=4, random_state=42, enable_categorical=True) # multi class returns propabilites\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(xgb_model, X_train, X_test, y_train, y_test)\n",
    "print_xgb_params(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HyperParameter Tuninig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier( n_estimators: int = 1, *, booster: typing.Literal[\"gbtree\", \"dart\"] = \"gbtree\", dart_normalized_type: typing.Literal[\"tree\", \"forest\"] = \"tree\", tree_method: typing.Literal[\"auto\", \"exact\", \"approx\", \"hist\"] = \"auto\", min_tree_child_weight: int = 1, colsample_bytree: float = 1.0, colsample_bylevel: float = 1.0, colsample_bynode: float = 1.0, gamma: float = 0.0, max_depth: int = 6, subsample: float = 1.0, reg_alpha: float = 0.0, reg_lambda: float = 1.0, learning_rate: float = 0.3, max_iterations: int = 20, tol: float = 0.01, enable_global_explain: bool = False, xgboost_version: typing.Literal[\"0.9\", \"1.1\"] = \"0.9\" )\n",
    "\n",
    "n_estimators - num_parallel_tree\n",
    "\n",
    "booster - gbtree (gradient boosting with trees) is generally a stronger choice for low-dimensional data. no need for  dart (Dropout Additive Regression Trees) and much slower\n",
    "\n",
    "tree_method - (Tree Construction Algorithms) didnt look better.\n",
    "\n",
    "min_child_weight: minimum sum of instance weight (hessian) needed in a child. This can be used to control the complexity of the decision tree by preventing the creation of too small leaves. tried [1, 5, 10] and didnt improve.\n",
    "\n",
    "colsample_bytree: percentage of columns used for each tree construction. Lowering this value can prevent overfitting by training on a subset of the features. we dont have overfitting, and when we trained 1 tree to take all features was best.\n",
    "same for by_tree and by_node\n",
    "\n",
    "gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. Higher values increase the regularization. \n",
    "\n",
    "**max_depth: [4, 6, 8]** Deeper trees can capture more complex patterns in the data, but may also lead to overfitting.\n",
    "\n",
    "subsample: percentage of rows used for each tree construction. Lowering this value can prevent overfitting by training on a smaller subset of the data.\n",
    "\n",
    "reg_alpha\t- L1 regularization term on weights (xgb's alpha). Default to 0.0.\n",
    "\n",
    "reg_lambda- L2 regularization term on weights (xgb's lambda). Default to 1.0.\n",
    "\n",
    "**learning_rate: [0.1, 0.3, 0.4]** deafulat 0.3\n",
    "\n",
    " max_iterations - would be redundant because early stopping already determines the optimal number of boosting rounds based on data and other hyperparameters.\n",
    "\n",
    " tol - Minimum relative loss improvement necessary to continue training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_xgboost_train_test_semi_categorical()\n",
    "\n",
    "xgb_model = XGBClassifier(objective='multi:softprob', num_class=4, random_state=42, enable_categorical=True)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8, 12],\n",
    "    'learning_rate': [0.05, 0.08, 0.1, 0.3, 0.4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "xgb_best_model = grid_search.best_estimator_\n",
    "evaluate_model(xgb_best_model, X_train, X_test, y_train, y_test)\n",
    "print_xgb_params(xgb_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_xgboost_train_test_all_categorical()\n",
    "\n",
    "xgb_model = XGBClassifier(objective='multi:softprob', num_class=4, random_state=42, enable_categorical=True)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8, 12],\n",
    "    'learning_rate': [0.05, 0.08, 0.1, 0.3, 0.4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "xgb_best_model = grid_search.best_estimator_\n",
    "evaluate_model(xgb_best_model, X_train, X_test, y_train, y_test)\n",
    "print_xgb_params(xgb_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C) Results Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Accuracy |\n",
    "|---|---|\n",
    "| Naive Bayes | 0.34 |\n",
    "| Decision Tree | 0.42 |\n",
    "| Random Forest | 0.37 |\n",
    "| XGBoost | 0.42 |\n",
    "\n",
    "The fact that only a single decision tree surpasssed random forest and XGBoost was surprising.\n",
    "\n",
    "**Observations:**\n",
    "* **Decision Tree and XGBoost:** Both Decision Tree and XGBoost achieved the highest accuracy of 0.42. This suggests that these models were able to learn more complex patterns in the data compared to Naive Bayes and Random Forest.\n",
    "* **Naive Bayes:** Naive Bayes had the lowest accuracy at 0.34. This might indicate that the assumption of feature independence made by Naive Bayes doesn't hold well for your data.\n",
    "\n",
    "\n",
    "We couldnt surpass 42%. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D) Most Effective Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most effective parameters as we explained before were max_depth - because the tree grew too large becuase we have a lot of data. also max_leaf_nodes which capped the tree size did a good job - accuracy went up by 15% and reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.E) Statistical Anylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate_models(nb_best_model, dt_best_model_2, rf_best_model, xgb_best_model)\n",
    "\n",
    "def evaluate_multiclass_model(model, X_test, y_test, model_name, n_classes):\n",
    "    # Predict probabilities for ROC curve\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Binarize the output for ROC\n",
    "    y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "    \n",
    "    # Initialize dictionaries to store metrics\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_prob.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Predict labels for other metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate per-class FPR and FNR\n",
    "    fpr_per_class = []\n",
    "    fnr_per_class = []\n",
    "    for i in range(n_classes):\n",
    "        tn = np.sum(cm[:, :i].sum() + cm[:, i+1:].sum())  # True Negatives for class i\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]  # False Positives for class i\n",
    "        fn = cm[i, :].sum() - cm[i, i]  # False Negatives for class i\n",
    "        tp = cm[i, i]  # True Positives for class i\n",
    "        fpr_per_class.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "        fnr_per_class.append(fn / (fn + tp) if (fn + tp) > 0 else 0)\n",
    "    \n",
    "    # Compute global metrics (macro average)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nMetrics for {model_name}:\")\n",
    "    print(f\"ROC AUC (Micro): {roc_auc['micro']:.4f}\")\n",
    "    print(f\"Per-class ROC AUC: {[f'{i}: {roc_auc[i]:.4f}' for i in range(n_classes)]}\")\n",
    "    print(f\"Per-class FPR: {[f'{i}: {fpr_per_class[i]:.4f}' for i in range(n_classes)]}\")\n",
    "    print(f\"Per-class FNR: {[f'{i}: {fnr_per_class[i]:.4f}' for i in range(n_classes)]}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "    \n",
    "    return fpr, tpr, roc_auc, y_pred, cm\n",
    "\n",
    "models = {\n",
    "    \"best_nb\": nb_best_model,\n",
    "    \"best_dt\": dt_best_model_2,\n",
    "    \"best_rf\": rf_best_model,\n",
    "    \"best_xgb\": xgb_best_model,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    fpr, tpr, roc_auc, y_pred, cm = evaluate_multiclass_model(model, X_test, y_test, name, 4)\n",
    "    results[name] = {\"fpr\": fpr, \"tpr\": tpr, \"roc_auc\": roc_auc, \"y_pred\": y_pred, \"cm\": cm}\n",
    "\n",
    "# Plot ROC curves for all models (micro-average)\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "for name, result in results.items():\n",
    "    plt.plot(result[\"fpr\"][\"micro\"], result[\"tpr\"][\"micro\"], \n",
    "             color=next(colors), label=f'{name} (AUC = {result[\"roc_auc\"][\"micro\"]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Micro-average ROC Curve Comparison (Multiclass)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print a summary table (macro averages)\n",
    "print(\"\\nSummary Table:\")\n",
    "print(\"Model     | AUC (Micro) | Accuracy | Precision | Recall | F1 Score\")\n",
    "print(\"-\" * 70)\n",
    "for name, result in results.items():\n",
    "    y_pred = result[\"y_pred\"]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"{name:9} | {result['roc_auc']['micro']:.4f} | {accuracy:.4f} | {precision:.4f} | {recall:.4f} | {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Clustering Based on Price Polices \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Identify the 150 hotels with the most data in the dataset and extract their records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./hotels_data_changed.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "hotel_counts = df['Hotel Name'].value_counts()\n",
    "top_150_hotels = hotel_counts.head(150).index\n",
    "filtered_df = df[df['Hotel Name'].isin(top_150_hotels)]\n",
    "\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Find the 40 most common check-in dates  in the dataset and extract their records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkin_counts = filtered_df['Checkin Date'].value_counts()\n",
    "top_40_checkin_dates = checkin_counts.head(40).index\n",
    "filtered_checkin_df = filtered_df[filtered_df['Checkin Date'].isin(top_40_checkin_dates)]\n",
    "\n",
    "display(filtered_checkin_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 160-dimensional feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Build a 160-dimensional feature vector for each hotel based on its discount pricing behavior. Each vector is constructed by:\n",
    "- Filtering the top 150 hotels (by record count) and the top 40 checkin dates.\n",
    "- For each hotel, extracting 4 discount prices (one per discount code) for each of the 40 checkin dates. If several snapshots exist, we will select the one's with minimal prices.\n",
    "- If no data is available for a specific (checkin date, discount code) combination, mark it with `-1`.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "1. **Group the Data:**  \n",
    "   Group the filtered data by **Hotel Name**, **Checkin Date**, and **Discount Code**. For each group, compute the minimum discount price, ensuring that only the best (lowest) price per combination is selected.\n",
    "\n",
    "2. **Pivot to Wide Format:**  \n",
    "   Transform the grouped data into a wide format where:\n",
    "   - Each row represents a single hotel.\n",
    "   - Each column represents a unique (Checkin Date, Discount Code) combination, totaling 160 columns (40 dates × 4 codes).\n",
    "\n",
    "3. **Fill Missing Data:**  \n",
    "   - Reindex the pivoted DataFrame so that every hotel has all 160 combinations, filling missing entries with `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group by Hotel Name, Checkin Date, and Discount Code and select the minimum Discount Price.\n",
    "grouped = (\n",
    "    filtered_checkin_df\n",
    "    .groupby(['Hotel Name', 'Checkin Date', 'Discount Code'])['Discount Price']\n",
    "    .min()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 2. Pivot the DataFrame so that:\n",
    "#    - The index is 'Hotel Name'\n",
    "#    - The columns are a MultiIndex with levels (Checkin Date, Discount Code)\n",
    "#    - The values are the minimum discount prices.\n",
    "pivot_df = grouped.pivot_table(index='Hotel Name',\n",
    "                               columns=['Checkin Date', 'Discount Code'],\n",
    "                               values='Discount Price')\n",
    "\n",
    "\n",
    "# 3. Reindex the columns so that all 40 checkin dates and 4 discount codes are present.\n",
    "#    Use the top_40_checkin_dates (from your earlier filtering) and the list [1, 2, 3, 4] for discount codes. \n",
    "all_combinations = pd.MultiIndex.from_product([top_40_checkin_dates, [1, 2, 3, 4]],\n",
    "                                                names=['Checkin Date', 'Discount Code'])\n",
    "\n",
    "pivot_df = pivot_df.reindex(columns=all_combinations, fill_value=-1)\n",
    "pivot_df = pivot_df.fillna(-1)\n",
    "\n",
    "\n",
    "pivot_df.columns = [\n",
    "    col if isinstance(col, str) else f\"{col[0]} - {col[1]}\"\n",
    "    for col in pivot_df.columns\n",
    "]\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "print(pivot_df.shape[0]) # Note we have 149 hotels instead of 150 - solution in next cell\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Missing Hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verifying Missing Hotel Data**\n",
    "\n",
    "After filtering and pivoting the data, we expect to have 150 hotels, but only 149 appear in our pivot table. This indicates that one (or more) of the top 150 hotels has no records for the top 40 check-in dates used in our analysis.\n",
    "\n",
    "The code above does the following:\n",
    "1. **Identify Missing Hotels:**  \n",
    "   It compares the complete list of top 150 hotels (`top_150_hotels`) with the hotel names present in the pivoted DataFrame (`pivot_df`). Any hotel that is not present is added to the `missing_hotels` list.\n",
    "\n",
    "2. **Check Data for Each Missing Hotel:**  \n",
    "   For each missing hotel, it filters `filtered_checkin_df` (which already contains only records from the top 40 check-in dates) to see if there are any records for that hotel.  \n",
    "   - If the resulting DataFrame is empty, it confirms that the hotel indeed has no data for those check-in dates.  \n",
    "   - This explains why the hotel did not appear in the pivot table.\n",
    "\n",
    "By verifying that the missing hotel has no records in the filtered data, we can conclude that the drop in the number of hotels is due to the absence of data for those check-in dates rather than an error in our processing pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already defined:\n",
    "# - top_150_hotels: the complete list of top 150 hotel names.\n",
    "# - pivot_df: the pivoted DataFrame after grouping and filtering.\n",
    "# - filtered_checkin_df: the DataFrame filtered by top 40 check-in dates.\n",
    "#\n",
    "# And the missing hotels are identified as:\n",
    "missing_hotels = [hotel for hotel in top_150_hotels if hotel not in pivot_df['Hotel Name'].values]\n",
    "print(\"Missing hotels:\", missing_hotels)\n",
    "\n",
    "# For each missing hotel, check if there is any record in the filtered_checkin_df.\n",
    "for hotel in missing_hotels:\n",
    "    hotel_records = filtered_checkin_df[filtered_checkin_df['Hotel Name'] == hotel]\n",
    "    print(f\"\\nRecords for missing hotel '{hotel}':\")\n",
    "    print(hotel_records)  # This should print an empty DataFrame if no data is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Normalize 0-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Task**\n",
    "\n",
    "For each hotel, we have a 160-dimensional vector of discount prices (one for each combination of Checkin Date and Discount Code). The goal is to normalize these prices so that, for each hotel, the lowest valid discount price becomes 0 and the highest becomes 100. Any missing value (indicated by `-1`) should remain unchanged.\n",
    "\n",
    "**Plan**\n",
    "\n",
    "1. **Define a Normalization Function:**  \n",
    "   Create a function (`normalize_row`) that:\n",
    "   - Filters out the missing values (`-1`) from the row.\n",
    "   - Computes the minimum and maximum values among the valid discount prices.\n",
    "   - Applies the normalization formula:\n",
    "     $$\n",
    "     \\text{normalized\\_price} = \\frac{(\\text{price} - \\text{min\\_price})}{(\\text{max\\_price} - \\text{min\\_price})} \\times 100\n",
    "     $$\n",
    "   - Handles the case where all valid prices are equal (to avoid division by zero) by setting them to 0.\n",
    "\n",
    "2. **Apply the Function Row-wise:**  \n",
    "   Normalize the discount prices for each hotel (i.e., for each row) by applying the function to all columns except the \"Hotel Name\".\n",
    "\n",
    "3. **Round and Convert to Integers:**  \n",
    "   After normalization, round the values to the nearest integer and convert them to an integer type, ensuring that the normalized prices are stored as integers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_row(row):\n",
    "    valid_mask = row != -1\n",
    "    valid_prices = row[valid_mask]\n",
    "    \n",
    "    if valid_prices.empty:\n",
    "        return row\n",
    "    \n",
    "    min_price = valid_prices.min()\n",
    "    max_price = valid_prices.max()\n",
    "    \n",
    "   # Avoid division by zero if all valid prices are identical\n",
    "    if min_price == max_price:\n",
    "        row[valid_mask] = 0\n",
    "    else:\n",
    "        # Compute the normalized values, round them, and cast to int\n",
    "        normalized_values = ((row[valid_mask] - min_price) / (max_price - min_price)) * 100\n",
    "        row[valid_mask] = normalized_values.round(0).astype(int)\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "pivot_df.iloc[:, 1:] = pivot_df.iloc[:, 1:].apply(normalize_row, axis=1)\n",
    "\n",
    "for col in pivot_df.columns[1:]:\n",
    "    pivot_df[col] = pd.to_numeric(pivot_df[col], errors='coerce')\n",
    "    pivot_df[col] = pivot_df[col].astype(\"Int64\")\n",
    "\n",
    "\n",
    "display(pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_clustering_data = \"./hotels_clustering_data.csv\"\n",
    "pivot_df.to_csv(hotels_clustering_data, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**\n",
    "\n",
    "Using the normalized discount prices for each hotel, we will perform hierarchical clustering to group hotels that exhibit similar pricing behaviors. We have a 160-dimensional feature vector for each hotel (each dimension corresponds to a specific (Checkin Date, Discount Code) pair).\n",
    "\n",
    "**Plan**\n",
    "\n",
    "1. **Prepare the Data:**  \n",
    "   - Load the saved CSV file (`hotels_clustering_data.csv`).\n",
    "   - Separate the \"Hotel Name\" column (for labeling) from the numeric feature columns.\n",
    "\n",
    "2. **Perform Hierarchical Clustering:**  \n",
    "   - Use SciPy's `linkage` function with Ward's method (which works well with Euclidean distance) to compute the clustering.\n",
    "   - Generate a linkage matrix that represents the hierarchical clustering.\n",
    "\n",
    "3. **Plot the Dendrogram:**  \n",
    "   - Use SciPy's `dendrogram` function to visualize the hierarchical clustering.\n",
    "   - Label each leaf in the dendrogram with the corresponding hotel name to help interpret the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "def create_dendrogram_from_csv(csv_path, color_threshold=825, width=1200, height=1800):\n",
    "\n",
    "    clu_df = pd.read_csv(csv_path)\n",
    "    hotel_names = clu_df[\"Hotel Name\"].values\n",
    "    X = clu_df.drop(\"Hotel Name\", axis=1).values\n",
    "    Z = linkage(X, method='ward')\n",
    "\n",
    "    fig = ff.create_dendrogram(\n",
    "        X,\n",
    "        orientation='left',\n",
    "        labels=hotel_names,\n",
    "        color_threshold=color_threshold,\n",
    "        linkagefun=lambda x: Z\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=width,\n",
    "        height=height\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "color_threshold_list = [825, 750, 625, 500]\n",
    "for color_threshold in color_threshold_list:\n",
    "    print (f'color_threshold={color_threshold}')\n",
    "    create_dendrogram_from_csv(\"hotels_clustering_data.csv\", color_threshold=color_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed hierarchical clustering on a dataset of hotels, where each hotel is represented by a **160-dimensional vector** of normalized discount prices. In simpler terms, each hotel’s vector shows *how* it discounts (and by how much) across different dates and discount codes. The dendrogram below clusters these hotels based on their similarity in discounting patterns.\n",
    "\n",
    "Below, we examine **five different “cut” distances**—825, 750, 625, 500, and an additional view with fewer, broader clusters—and describe the cluster/subgroup formations you see in each figure.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Explanation of the Dendrogram\n",
    "- **X-axis**: The distance (or dissimilarity) at which clusters merge. Larger values mean more dissimilar groups.  \n",
    "- **Y-axis**: The list of hotels, labeled along the left side.  \n",
    "- **Colored Branches**: Each color indicates a cluster or subgroup under the specified distance threshold.\n",
    "\n",
    "In general:  \n",
    "- Hotels that **merge at smaller distances** (farther to the left in the dendrogram) are quite similar in how they price their discounts.  \n",
    "- If you follow the dendrogram to the right until a major branch merges, that indicates hotels (or clusters of hotels) that are more dissimilar in their pricing behavior.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Cut at Distance ≈ 825\n",
    "\n",
    "![Cut at ~825](./images/clustering-825.png)  \n",
    "\n",
    "- We see **three main clusters** forming at this high distance threshold.  \n",
    "- **Group 1** (green), **Group 2** (red), and **Group 3** (blue) represent broad differences in discounting strategies. \n",
    "- Within each group, hotels share overall similarities in their discount patterns, but we’re not yet seeing the finer differences.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Cut at Distance ≈ 750\n",
    "\n",
    "![Cut at ~750](./images/clustering-750.png)  \n",
    "\n",
    "- Lowering the threshold to **~750** begins to reveal **subgroups** within **Groups 1** and **3**, while **Group 2** remains mostly intact.  \n",
    "- **Group 1** (previously green) starts splitting into multiple smaller clusters. These subgroups suggest that, while these hotels share a broad discount pattern, some differences in exact pricing behavior are now visible.  \n",
    "- **Group 3** also subdivides more than before.\n",
    "- **Group 2** is still one cluster, which indicates that these hotels are, as a whole, more cohesive or less varied in their discount strategies at this distance.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Cut at Distance ≈ 625\n",
    "\n",
    "![Cut at ~625](./images/clustering-625.png)  \n",
    "\n",
    "- By **~625**, **Group 1** and **Group 3** break into several distinct subgroups, illustrating more granular differences.  \n",
    "- **Group 2** is still a single cluster—indicating even at this more fine-grained level, the hotels in Group 2 remain very similar.  \n",
    "- This suggests **Group 2** has a stable, uniform discount pattern, or at least they differ less from each other than from the other groups.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Cut at Distance ≈ 500\n",
    "\n",
    "![Cut at ~500](./images/clustering-500.png)  \n",
    "\n",
    "- Finally, at **~500**, **Group 2** splits into **two subgroups**, showing that there are *at least* two distinct discounting patterns within Group 2 when we look closely.  \n",
    "- **Groups 1 and 3** now fragment into many smaller subgroups. If you count them, there could be several mini-clusters in each.  \n",
    "- Each **subgroup** here represents hotels that are very closely aligned in how they handle discounts across time—likely direct competitors or hotels following near-identical pricing guidelines.\n",
    "\n",
    "---\n",
    "\n",
    "#### Meaning of Subgroups\n",
    "\n",
    "1. **Competitive Landscape**  \n",
    "   - Hotels in the **same subgroup** are likely *direct competitors*, as they share nearly identical discount structures and timelines.\n",
    "\n",
    "2. **Revenue Management Strategy**  \n",
    "   - Subgroups often align with brand or chain policies (e.g., a single chain might appear as a tight cluster if they use centralized pricing software).  \n",
    "   - The level of granularity around **500** distance reveals the *very fine details* of each hotel’s promotional activities.\n",
    "\n",
    "3. **Marketing & Differentiation**  \n",
    "   - If you manage a hotel in these clusters, you might examine how close neighbors in the dendrogram are discounting.  \n",
    "   - Being in a tight cluster might motivate you to **differentiate** or **align** your pricing further.\n",
    "\n",
    "4. **Insights for Group 2**  \n",
    "   - Group 2’s stability up to a lower threshold (625) suggests a coherent pricing approach among its hotels. They only start splitting at ~500, which indicates that their differences are more subtle and only become apparent under a finer lens.\n",
    "\n",
    "---\n",
    "\n",
    "#### Putting It All Together\n",
    "At **higher thresholds** (825+), we see broad strokes: *three main pricing behaviors*.  As we lower the cut distance, we discover **finer distinctions** within each major group:\n",
    "- **Groups 1 & 3** develop several distinct subgroups earlier (around 750 or 625).  \n",
    "- **Group 2** remains consistent longer and only splits around 500, revealing that even seemingly uniform clusters can exhibit small internal differences if you zoom in far enough.\n",
    "\n",
    "In essence, **each subgroup** helps us understand small sets of hotels that might share very close discounting patterns—key information for competitive analysis and strategic pricing decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diving even deeper - (checking the hotel stars and average price with the pricing strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the last section were interesting, so i decided to run the same clustering but with the hotels stars, prices and discounts to see if some patterns emerge.\n",
    "\n",
    "The new label now contains:\n",
    "- stars\n",
    "- avg price\n",
    "- avg discount\n",
    "- avg discount rate \n",
    "\n",
    "in this format:\n",
    "\n",
    "`(stars) - price - discount - discount rate`\n",
    "\n",
    "example:\n",
    "\n",
    "(5) - 3898 - 3663 - 6% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pivot_df = pd.read_csv(\"hotels_clustering_data.csv\")\n",
    "df = pd.read_csv(\"./hotels_data_changed.csv\")\n",
    "\n",
    "pivot_df[\"Hotel Name\"] = pivot_df[\"Hotel Name\"].astype(str).str.strip()\n",
    "hotel_counts = df[\"Hotel Name\"].value_counts()\n",
    "top_150_hotels = hotel_counts.head(150).index\n",
    "\n",
    "summary_df = (\n",
    "    df[df[\"Hotel Name\"].isin(top_150_hotels)]\n",
    "    .groupby(\"Hotel Name\")\n",
    "    .agg({\"Original Price\": \"mean\", \"Discount Price\": \"mean\", \"Hotel Stars\": \"first\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary_df[\"Hotel Name\"] = summary_df[\"Hotel Name\"].astype(str).str.strip()\n",
    "summary_df[\"Original Price\"] = summary_df[\"Original Price\"].round(0).astype(int)\n",
    "summary_df[\"Discount Price\"] = summary_df[\"Discount Price\"].round(0).astype(int)\n",
    "merged_df = pivot_df.merge(summary_df, on=\"Hotel Name\", how=\"left\")\n",
    "\n",
    "merged_df[\"DiscountPerc\"] = (((merged_df[\"Original Price\"] - merged_df[\"Discount Price\"]) / merged_df[\"Original Price\"]) * 100).round(0).astype(int)\n",
    "merged_df[\"Label\"] = merged_df.apply(lambda row: f\"({row['Hotel Stars']}) - {row['Original Price']} - {row['Discount Price']} - {row['DiscountPerc']}%\", axis=1)\n",
    "\n",
    "merged_df.head()\n",
    "merged_df.to_csv(\"./hotels_clustering_data_with_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "def create_dendrogram_from_csv(csv_path, color_threshold=825, width=1200, height=1800):\n",
    "    clu_df = pd.read_csv(csv_path)\n",
    "    if \"Label\" in clu_df.columns:\n",
    "        labels = clu_df[\"Label\"].values\n",
    "        non_clustering = [\"Hotel Name\", \"Label\", \"Hotel Stars\", \"Original Price\", \"Discount Price\", \"DiscountPerc\"]\n",
    "    else:\n",
    "        labels = clu_df[\"Hotel Name\"].values\n",
    "        non_clustering = [\"Hotel Name\", \"Hotel Stars\", \"Original Price\", \"Discount Price\", \"DiscountPerc\"]\n",
    "    X = clu_df.drop(columns=non_clustering, errors='ignore').values\n",
    "    Z = linkage(X, method='ward')\n",
    "    fig = ff.create_dendrogram(\n",
    "        X,\n",
    "        orientation='left',\n",
    "        labels=labels,\n",
    "        color_threshold=color_threshold,\n",
    "        linkagefun=lambda x: Z\n",
    "    )\n",
    "    fig.update_layout(width=width, height=height)\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "color_threshold_list = [825, 750, 625, 500]\n",
    "for color_threshold in color_threshold_list:\n",
    "    print(f'color_threshold={color_threshold}')\n",
    "    create_dendrogram_from_csv(\"hotels_clustering_data_with_summary.csv\", color_threshold=color_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering with Star Rating, Avg Price, and Discounts\n",
    "\n",
    "We re-ran clustering with four features:\n",
    "1. **Hotel star rating**  \n",
    "2. **Avg nightly price**  \n",
    "3. **Avg absolute discount**  \n",
    "4. **Avg discount rate (%)**\n",
    "\n",
    "Each dendrogram label is `(stars) – price – discount – discount rate`.\n",
    "\n",
    "#### 1. Main Observations\n",
    "\n",
    "- **Star Rating & Price** dominate the first major splits:\n",
    "  - **Budget/Midscale** (2–3★, ~1,200–1,800 price) form one big group.\n",
    "  - **Upscale/Luxury** (4–5★, 2,500–4,000+) cluster separately.\n",
    "- **Discount Patterns** refine clusters within each star tier:\n",
    "  - Hotels sharing similar price but different discount behavior split into distinct subgroups.\n",
    "  - Aggressive vs. conservative discounters separate at lower distance thresholds.\n",
    "\n",
    "#### 2. Cluster Insights\n",
    "\n",
    "1. **Large Distance Clusters**:\n",
    "   - Budget vs. Luxury hotels split clearly by base price and star rating.\n",
    "   - Some **mixed star** groups appear if their price/discount overlap.\n",
    "\n",
    "2. **Subgroups at Tighter Distances**:\n",
    "   - Within the same star tier, variations in discount size or percentage form separate sub-clusters.\n",
    "   - Outliers can include a high-star hotel with heavy discounts or a low-star with high pricing.\n",
    "\n",
    "#### 3. Takeaways\n",
    "\n",
    "- **Star Rating is Primary**; discount strategies refine subgroups.  \n",
    "- Hotels in tight clusters likely share near-identical pricing/discount policies, suggesting **direct competition**.  \n",
    "- Examine outliers for unique pricing approaches or potential repositioning strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4 - Building a Predictive Model for Hotel Pricing Dynamics Using Snapshot Data (Deprecated - Task was unclear)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Breakdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Objective:**  \n",
    "  Predict the `Discount Price` for check-in dates within 30 days of a given `Snapshot Date` while minimizing the number of price samples (queries) from the database.\n",
    "\n",
    "- **Context:**  \n",
    "  Given a dataset with fields like `Snapshot Date`, `Hotel Name`, `Discount Code`, and various price-related features, our task is to build a model that can infer the price trends and predict missing prices accurately.\n",
    "\n",
    "- **Performance Measure:**  \n",
    "  The model's success is evaluated by the R-squared metric on a subset of 40 hotels with the most complete data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Approaches: Gaussian Process Regression vs. Bayesian Linear and Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gaussian Process Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - **Uncertainty Quantification:** Provides both predictions and confidence intervals, which is key for active sampling.\n",
    "  - **Sample Efficiency:** Excels when data is limited, helping us select the most informative samples.\n",
    "  - **Flexibility:** Capable of modeling complex, non-linear relationships inherent in price trends.\n",
    "  \n",
    "- **Cons:**\n",
    "  - **Computational Cost:** Can become computationally expensive with larger datasets.\n",
    "  - **Kernel Selection:** Requires careful tuning of the kernel functions to capture the underlying data patterns accurately.\n",
    "\n",
    "- **Fit to the Problem:**  \n",
    "  Ideal for our task since its uncertainty estimates allow us to strategically choose the next check-in dates to sample, ensuring we use as few queries as possible while maintaining high prediction performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Bayesian Linear and Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - **Simplicity:** Easy to implement and interpret.\n",
    "  - **Speed:** Generally faster to train compared to GP, which can be advantageous with simpler or smooth trends.\n",
    "  \n",
    "- **Cons:**\n",
    "  - **Limited Flexibility:** May not capture complex non-linear patterns as effectively.\n",
    "  - **Uncertainty Estimates:** While Bayesian methods provide uncertainty measures, they are often less robust in capturing the variability in complex datasets compared to GP.\n",
    "\n",
    "- **Fit to the Problem:**  \n",
    "  Suitable if the price trends were very smooth and predictable. However, the nuances in hotel pricing (e.g., varying discount strategies, day-of-week effects) suggest a need for a more flexible approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Chosen Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Gaussian Process Regression** is selected as the primary method. Its ability to provide uncertainty estimates and guide active sampling makes it the best fit for minimizing the number of price queries while still achieving a high R-squared on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Date Encoding:**  \n",
    "  Convert `Snapshot Date` and `Checkin Date` into numerical features (e.g., days from the snapshot, day-of-week, etc.) to capture temporal patterns.\n",
    "\n",
    "- **Handling Categorical Data:**  \n",
    "  Apply one-hot encoding or similar methods to features such as `Hotel Name` and `Discount Code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./hotels_data_changed.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data['OG_Hotel_Name'] = data['Hotel Name']\n",
    "\n",
    "data = feature_engineering(data)\n",
    "cols_to_drop = ['Snapshot Date', 'Checkin Date', 'WeekDay']\n",
    "data = data.drop(columns=cols_to_drop)\n",
    "\n",
    "# For our modeling purposes, we want to encode categorical variables.\n",
    "# Here, we choose 'Hotel Name' and 'Discount Code' as categorical features.\n",
    "categorical_features = ['Hotel Name', 'Discount Code']\n",
    "data = encode_categorical_columns(data, categorical_features)\n",
    "\n",
    "\n",
    "print(data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization/Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale continuous features as needed to ensure all features contribute equally to the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_to_scale = [\n",
    "    'Snapshot_Date_num',  \n",
    "    'Checkin_Date_num',   \n",
    "    'Original Price',\n",
    "    'Discount Price',\n",
    "    'DiscountDiff',\n",
    "    'DiscountPerc',\n",
    "    'Available Rooms',\n",
    "    'DayDiff'\n",
    "]\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data, fitted_scaler = normalize_features(data, features_to_scale)\n",
    "normalized_data[features_to_scale].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data split (Train/Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " Split the data into train and test Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_counts = normalized_data['OG_Hotel_Name'].value_counts()\n",
    "top_40_hotels = hotel_counts.head(40).index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "test_data = normalized_data[normalized_data['OG_Hotel_Name'].isin(top_40_hotels)]\n",
    "train_data = normalized_data[~normalized_data['OG_Hotel_Name'].isin(top_40_hotels)]\n",
    "\n",
    "test_data = test_data.drop(columns=['OG_Hotel_Name'])\n",
    "train_data = train_data.drop(columns=['OG_Hotel_Name'])\n",
    "\n",
    "target_variable = 'Discount Price'\n",
    "\n",
    "X_test = test_data.drop(columns=[target_variable])\n",
    "y_test = test_data[target_variable]\n",
    "\n",
    "X_train = train_data.drop(columns=[target_variable])\n",
    "y_train = train_data[target_variable]\n",
    "\n",
    "\n",
    "print(\"Test set (features):\", X_test.shape)\n",
    "print(\"Test set (target):\", y_test.shape)\n",
    "print(\"Training set (features):\", X_train.shape)\n",
    "print(\"Training set (target):\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression: Training, Testing, and Approach Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "initial_sample_size = 30\n",
    "all_indices = np.array(X_train.index)\n",
    "initial_sample_indices = np.random.choice(all_indices, size=initial_sample_size, replace=False)\n",
    "sampled_indices = list(initial_sample_indices)\n",
    "candidate_indices = list(set(all_indices) - set(sampled_indices))\n",
    "\n",
    "# Define Gaussian Process Regression model with a basic kernel\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5)\n",
    "\n",
    "max_iterations = 50  # Maximum number of iterations (samples to add)\n",
    "r2_scores = []\n",
    "\n",
    "# --- Iterative Sampling ---\n",
    "for i in range(max_iterations):\n",
    "    # Train GP on current sampled data\n",
    "    X_sampled = X_train.loc[sampled_indices]\n",
    "    y_sampled = y_train.loc[sampled_indices]\n",
    "    gp.fit(X_sampled, y_sampled)\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    y_pred, sigma = gp.predict(X_test, return_std=True)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "    print(f\"Iteration {i+1}: Samples used = {len(sampled_indices)}, Test R² = {r2:.3f}\")\n",
    "    \n",
    "    # Break if no more candidates are left\n",
    "    if not candidate_indices:\n",
    "        break\n",
    "\n",
    "    # Predict on candidate pool to obtain uncertainty (standard deviation)\n",
    "    X_candidates = X_train.loc[candidate_indices]\n",
    "    _, sigma_candidates = gp.predict(X_candidates, return_std=True)\n",
    "    \n",
    "    # Select the candidate with the maximum uncertainty\n",
    "    max_uncertainty_idx = np.argmax(sigma_candidates)\n",
    "    new_sample_idx = X_candidates.index[max_uncertainty_idx]\n",
    "    \n",
    "    # Add this candidate to our sampled set and remove from candidates\n",
    "    sampled_indices.append(new_sample_idx)\n",
    "    candidate_indices.remove(new_sample_idx)\n",
    "\n",
    "print(\"Final number of samples used:\", len(sampled_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4 - Building a Predictive Model for Hotel Pricing Dynamics Using Snapshot Data (V2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Objective:**  \n",
    "  Predict the `Discount Price` for check-in dates within 30 days of a given `Snapshot Date` while minimizing the number of price samples (queries) from the database.\n",
    "\n",
    "- **Context:**  \n",
    "  Given fields of `Snapshot Date`, `Hotel Name`, `Discount Code`, build a relevant data set to predict the `Discount Price` for check-in dates within 30 days of the `Snapshot Date`.\n",
    "\n",
    "- **Performance Measure:**  \n",
    "  The model's success is evaluated by the R-squared metric on a subset of 40 hotels with the most complete data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Approaches: Gaussian Process Regression vs. Bayesian Linear and Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gaussian Process Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - **Uncertainty Quantification:** Provides both predictions and confidence intervals, which is key for active sampling.\n",
    "  - **Sample Efficiency:** Excels when data is limited, helping us select the most informative samples.\n",
    "  - **Flexibility:** Capable of modeling complex, non-linear relationships inherent in price trends.\n",
    "  \n",
    "- **Cons:**\n",
    "  - **Computational Cost:** Can become computationally expensive with larger datasets.\n",
    "  - **Kernel Selection:** Requires careful tuning of the kernel functions to capture the underlying data patterns accurately.\n",
    "\n",
    "- **Fit to the Problem:**  \n",
    "  Ideal for our task since its uncertainty estimates allow us to strategically choose the next check-in dates to sample, ensuring we use as few queries as possible while maintaining high prediction performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Bayesian Linear and Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Pros:**\n",
    "  - **Simplicity:** Easy to implement and interpret.\n",
    "  - **Speed:** Generally faster to train compared to GP, which can be advantageous with simpler or smooth trends.\n",
    "  \n",
    "- **Cons:**\n",
    "  - **Limited Flexibility:** May not capture complex non-linear patterns as effectively.\n",
    "  - **Uncertainty Estimates:** While Bayesian methods provide uncertainty measures, they are often less robust in capturing the variability in complex datasets compared to GP.\n",
    "\n",
    "- **Fit to the Problem:**  \n",
    "  Suitable if the price trends were very smooth and predictable. However, the nuances in hotel pricing (e.g., varying discount strategies, day-of-week effects) suggest a need for a more flexible approach.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Chosen Approach:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Gaussian Process Regression** is selected as the primary method. Its ability to provide uncertainty estimates and guide active sampling makes it the best fit for minimizing the number of price queries while still achieving a high R-squared on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the Data:**  \n",
    "   We read the CSV file into a DataFrame and ensure that date columns are properly converted to datetime objects.\n",
    "\n",
    "2. **Filter the Data:**  \n",
    "   We filter the dataset to include only rows where `DayDiff` is 30 or less, meaning the check-in is within 30 days after the snapshot date.\n",
    "\n",
    "3. **Group the Data:**  \n",
    "   We group by `Snapshot Date`, `Hotel Name`, and `Discount Code`, and count the number of transactions (rows) in each group. This count is stored in the column `transaction_count`.\n",
    "\n",
    "4. **Dataset: onePerHotel:**  \n",
    "   - For each hotel, we sort by `transaction_count` in descending order and select the combination with the maximum count.  \n",
    "   - Then, we sort these unique hotel rows by `transaction_count` and take the top 40.\n",
    "\n",
    "5. **Dataset: mostData:**  \n",
    "   - We simply sort all the grouped combinations by `transaction_count` in descending order and select the top 40 combinations.\n",
    "\n",
    "6. **Extracting Parameters:**  \n",
    "   For each dataset, we extract a list of dictionaries (or \"params\") containing `Snapshot Date`, `Hotel Name`, and `Discount Code`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"./hotels_data_changed.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# We are only interested in transactions with a check-in date within 30 days after the snapshot.\n",
    "data_filtered = data[data['DayDiff'] <= 30].copy()\n",
    "\n",
    "data_filtered['Snapshot Date'] = pd.to_datetime(data_filtered['Snapshot Date']).dt.normalize()\n",
    "\n",
    "# We group by the combination of 'Snapshot Date', 'Hotel Name', and 'Discount Code', and count the number of transactions.\n",
    "grouped = data_filtered.groupby(\n",
    "    ['Snapshot Date', 'Hotel Name', 'Discount Code']\n",
    ").size().reset_index(name='transaction_count')\n",
    "\n",
    "# For each hotel, choose the combination with the highest transaction_count.\n",
    "# First, sort within each hotel so that the highest count is on top,\n",
    "# then group by 'Hotel Name' and take the first (best) row.\n",
    "onePerHotel = (\n",
    "    grouped.sort_values(['Hotel Name', 'transaction_count'], ascending=[True, False])\n",
    "    .groupby('Hotel Name')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "onePerHotel = onePerHotel.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "mostData = grouped.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "params_onePerHotel = onePerHotel[['Snapshot Date', 'Hotel Name', 'Discount Code']].to_dict('records')\n",
    "params_mostData   = mostData[['Snapshot Date', 'Hotel Name', 'Discount Code']].to_dict('records')\n",
    "\n",
    "print(\"onePerHotel (Top 40 hotels with best combination per hotel):\")\n",
    "display(onePerHotel.head())\n",
    "\n",
    "print(\"mostData (Top 40 combinations overall):\")\n",
    "display(mostData.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we would create a function that gets the `Snapshot Date`, `Hotel Name`, and `Discount Code` and returns all we need to start training the model.\n",
    "This includes the following:\n",
    "-  get relevant data from the dataset\n",
    "-  apply feature engineering on that data\n",
    "-  normalize and scale the data\n",
    "-  Splitting into X and y & Creating the Wrapper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_data(params, data):\n",
    " \n",
    "    snapshot_date = pd.to_datetime(params['Snapshot Date']).normalize()\n",
    "    \n",
    "    subset = data[\n",
    "        (data['Snapshot Date'] == snapshot_date) &\n",
    "        (data['Hotel Name'] == params['Hotel Name']) &\n",
    "        (data['Discount Code'] == params['Discount Code']) &\n",
    "        (data['DayDiff'] <= 30) \n",
    "    ]\n",
    "    return subset\n",
    "\n",
    "test_params = params_mostData[0]\n",
    "relevant_subset = load_relevant_data(test_params, data_filtered)\n",
    "print(\"Parameters:\", test_params)\n",
    "print(\"Relevant Data (first 5 rows):\")\n",
    "display(relevant_subset.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we transform our raw data into a format that is ready for model training. The goals are to:\n",
    "\n",
    "- **Convert Date Information:**  \n",
    "  We convert the date column `Checkin Date` into numerical values (Unix timestamps) so that the model can understand and work with temporal data.\n",
    "\n",
    "- **Encode Weekday Information:**  \n",
    "  We convert the weekday from a string (e.g., \"Monday\") into a numeric value using the helper function `convert_weekday_to_num`, storing it as `WeekDay_num`.\n",
    "\n",
    "- **Remove Irrelevant or Constant Columns:**  \n",
    "  After feature engineering, some columns become redundant or uninformative:\n",
    "  - **`Hotel Name`:** Removed because the data is filtered for a specific hotel, or if used for filtering, the raw name is no longer needed.\n",
    "  - **`Checkin Date`:** Once their numerical representations are extracted, the raw date values are no longer required.\n",
    "  - **`WeekDay`:** We already have the numeric `WeekDay_num`, making the original string column redundant.\n",
    "  - **`Days`:** This column is always the same (always 5), so it does not provide any variance or useful information for training.\n",
    "  - **`Snapshot Date`:** Since the snapshot is the same across the subset, this column is constant and does not help differentiate between records.\n",
    "  - **`Snapshot ID`:** This is also constant for a given snapshot and can be removed to reduce noise.\n",
    "\n",
    "The removal of these columns helps to simplify our dataset and ensures that the model is trained only on features that provide meaningful variation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_data = feature_engineering(relevant_subset.copy())\n",
    "\n",
    "print(\"Engineered Data\")\n",
    "display(engineered_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalize and scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are we doing?**  \n",
    "We use a `StandardScaler` to standardize the continuous features by removing the mean and scaling to unit variance. This process prevents features with larger numeric ranges from dominating the learning process.\n",
    "\n",
    "**Why are we doing it?**  \n",
    "Normalization is crucial in many machine learning algorithms—especially when using distance-based metrics or gradient descent—for faster convergence and improved performance.\n",
    "\n",
    "**Features to Scale:**  \n",
    "Based on our engineered dataset, we scale:\n",
    "- `Checkin_Date_num` (numerical representation of the check-in date)  \n",
    "- `Original Price`  \n",
    "- `Discount Price`  \n",
    "- `Available Rooms`  \n",
    "- `DayDiff`  \n",
    "- `DiscountDiff`  \n",
    "- `DiscountPerc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scale = [\n",
    "    \"Checkin_Date_num\", \n",
    "    \"Original Price\", \n",
    "    \"Discount Price\", \n",
    "    \"Available Rooms\", \n",
    "    \"DayDiff\", \n",
    "    \"DiscountDiff\", \n",
    "    \"DiscountPerc\"\n",
    "]\n",
    "\n",
    "normalized_data, fitted_scaler = normalize_features(engineered_data, features_to_scale)\n",
    "\n",
    "print(\"Normalized Data\")\n",
    "display(normalized_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Splitting into X and y & Creating the Wrapper Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we:\n",
    "\n",
    "- Split the normalized data into features (`X`) and target (`y`). Here, `y` is the scaled \"Discount Price\".  \n",
    "- Create a wrapper function that runs the entire preprocessing pipeline (from getting relevant data to normalization) and returns `X`, `y`, the fitted scaler, and a helper function to reverse the scaling for the target.  \n",
    "- The helper function uses the scaling parameters for \"Discount Price\" (found in the fitted scaler) so that later we can convert predictions back to their original values.\n",
    "\n",
    "> **Why Remove \"Discount Price\" from X?**  \n",
    "> The target variable should not be present in the features. By dropping \"Discount Price\" from the normalized data, we ensure that `X` contains only the input features while `y` contains the target variable.\n",
    "\n",
    "> **Reversing the Scaling:**  \n",
    "> Since the scaler is fitted on multiple columns, we locate the index corresponding to \"Discount Price\" in our `features_to_scale` list and use its mean and scale to convert scaled predictions back to their original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(params, data, sort=False):\n",
    "    relevant = load_relevant_data(params, data)\n",
    "    engineered = feature_engineering(relevant.copy())\n",
    "    normalized, scaler = normalize_features(engineered, features_to_scale)\n",
    "    \n",
    "    if sorted:\n",
    "        normalized = normalized.sort_values(by=\"Discount Price\")\n",
    "\n",
    "    X = normalized.drop(columns=[\"Discount Price\"])\n",
    "    y = normalized[\"Discount Price\"]\n",
    "\n",
    "    idx = features_to_scale.index(\"Discount Price\")\n",
    "    \n",
    "    def reverse_scaling(y_scaled):\n",
    "        return y_scaled * scaler.scale_[idx] + scaler.mean_[idx]\n",
    "    \n",
    "    return X, y, scaler, reverse_scaling\n",
    "\n",
    "X, y, scaler, reverse_scaling = prepare_training_data(test_params,data_filtered )\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"First 5 values of y (scaled):\", y.head().values)\n",
    "print(\"First 5 values of y (original):\", reverse_scaling(y.head().values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression: Training And Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we initialize our GaussianProcessRegressor model. We use a kernel that combines a ConstantKernel and an RBF kernel. \n",
    "\n",
    "- **GaussianProcessRegressor:**  \n",
    "  Provides both predictions and uncertainty estimates, which are essential for our active sampling approach.\n",
    "\n",
    "- **ConstantKernel:**  \n",
    "  Represents a constant bias in the function, serving as a baseline.\n",
    "\n",
    "- **RBF Kernel (Radial Basis Function):**  \n",
    "  Models smooth variations in the data, which is ideal for capturing trends in discount prices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "def initialize_model():\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=1e-2, normalize_y=True)\n",
    "    return gp\n",
    "\n",
    "model = initialize_model()\n",
    "print(\"Initialized Model:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Sampling Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum uncertainty based stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we implement the active sampling loop. The process is as follows:\n",
    "\n",
    "- Start with an initial set of samples (e.g., the first, middle, and last indices).\n",
    "- Iteratively fit the Gaussian Process model on the current training set.\n",
    "- Predict on the remaining (unsampled) data and obtain uncertainty estimates.\n",
    "- Select the candidate with the highest uncertainty and add it to the training set.\n",
    "- Compute the `R²` score to monitor performance.\n",
    "- Stop when the maximum uncertainty is below a threshold or when the maximum iterations are reached.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def active_sampling_loop(X, y, max_iterations=10, uncertainty_threshold=0.05, initial_sample_indices=None):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    if initial_sample_indices is None:\n",
    "        initial_sample_indices = [0, n_samples // 2, n_samples - 1]\n",
    "    \n",
    "    training_indices = set(initial_sample_indices)\n",
    "    candidate_indices = set(range(n_samples)) - training_indices\n",
    "    iteration_log = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Prepare current training data\n",
    "        X_train = X.iloc[sorted(list(training_indices))]\n",
    "        y_train = y.iloc[sorted(list(training_indices))]\n",
    "        \n",
    "        # Initialize and fit the Gaussian Process model\n",
    "        model = initialize_model()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on candidate data and obtain uncertainties\n",
    "        candidate_list = sorted(list(candidate_indices))\n",
    "        if not candidate_list:\n",
    "            break\n",
    "\n",
    "        X_candidates = X.iloc[candidate_list]\n",
    "        y_pred_candidates, std_candidates = model.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        # Find candidate with maximum uncertainty\n",
    "        max_std = np.max(std_candidates)\n",
    "        chosen_candidate_idx = candidate_list[np.argmax(std_candidates)]\n",
    "        \n",
    "        y_pred_full = model.predict(X)\n",
    "        current_r2 = r2_score(y, y_pred_full)\n",
    "        iteration_log.append({\n",
    "            'iteration': iteration,\n",
    "            'num_samples': len(training_indices),\n",
    "            'max_uncertainty': max_std,\n",
    "            'r2': current_r2\n",
    "        })\n",
    "        \n",
    "        \n",
    "        if max_std < uncertainty_threshold:\n",
    "            break\n",
    "        \n",
    "        # Add the most uncertain candidate to the training set\n",
    "        training_indices.add(chosen_candidate_idx)\n",
    "        candidate_indices.remove(chosen_candidate_idx)\n",
    "    \n",
    "    # Final training on the selected samples\n",
    "    X_final = X.iloc[sorted(list(training_indices))]\n",
    "    y_final = y.iloc[sorted(list(training_indices))]\n",
    "    final_model = initialize_model()\n",
    "    final_model.fit(X_final, y_final)\n",
    "    final_predictions = final_model.predict(X)\n",
    "    final_r2 = r2_score(y, final_predictions)\n",
    "    \n",
    "    return {\n",
    "        'final_model': final_model,\n",
    "        'final_predictions': final_predictions,\n",
    "        'final_r2': final_r2,\n",
    "        'iteration_log': iteration_log,\n",
    "        'total_samples_used': len(training_indices)\n",
    "    }\n",
    "\n",
    "results = active_sampling_loop(X, y, max_iterations=15, uncertainty_threshold=0.05)\n",
    "\n",
    "print(\"Final R² Score:\", results['final_r2'])\n",
    "print(\"Total Samples Used:\", results['total_samples_used'])\n",
    "print(\"Iteration Log:\")\n",
    "for log in results['iteration_log']:\n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum uncertainty and stagnant Iterations based stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we implement an enhanced active sampling loop. In addition to stopping when the maximum uncertainty is very low, we also monitor `R²` improvement. The loop will stop if either:\n",
    "- The model's uncertainty is very low (`max_std < uncertainty_threshold`), or\n",
    "- The uncertainty is moderately low (`max_std < higher_uncertainty_threshold`) **and** the `R²` improvement has been stagnant for a set number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling_loop_with_stagnant_iterations_stop(X, y, max_iterations=10, \n",
    "                                                        uncertainty_threshold=0.05, \n",
    "                                                        higher_uncertainty_threshold=0.1,\n",
    "                                                        initial_sample_indices=None, \n",
    "                                                        r2_improvement_threshold=1e-4, \n",
    "                                                        max_stagnant_iterations=3):\n",
    "    n_samples = X.shape[0]\n",
    "    if initial_sample_indices is None:\n",
    "        initial_sample_indices = [0, n_samples // 2, n_samples - 1]\n",
    "    \n",
    "    training_indices = set(initial_sample_indices)\n",
    "    candidate_indices = set(range(n_samples)) - training_indices\n",
    "    iteration_log = []\n",
    "    \n",
    "    previous_r2 = -np.inf  \n",
    "    stagnant_iterations = 0  \n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Prepare training data for the current iteration\n",
    "        X_train = X.iloc[sorted(list(training_indices))]\n",
    "        y_train = y.iloc[sorted(list(training_indices))]\n",
    "        \n",
    "        # Initialize and fit the model\n",
    "        model = initialize_model()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on candidate data to obtain uncertainties\n",
    "        candidate_list = sorted(list(candidate_indices))\n",
    "        if not candidate_list:\n",
    "            break\n",
    "\n",
    "        X_candidates = X.iloc[candidate_list]\n",
    "        y_pred_candidates, std_candidates = model.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        max_std = np.max(std_candidates)\n",
    "        chosen_candidate_idx = candidate_list[np.argmax(std_candidates)]\n",
    "        \n",
    "        \n",
    "        y_pred_full = model.predict(X)\n",
    "        current_r2 = r2_score(y, y_pred_full)\n",
    "        \n",
    "        \n",
    "        r2_improvement = current_r2 - previous_r2\n",
    "        if r2_improvement < r2_improvement_threshold:\n",
    "            stagnant_iterations += 1\n",
    "        else:\n",
    "            stagnant_iterations = 0\n",
    "        \n",
    "        previous_r2 = current_r2  \n",
    "        \n",
    "        iteration_log.append({\n",
    "            'iteration': iteration,\n",
    "            'num_samples': len(training_indices),\n",
    "            'max_uncertainty': max_std,\n",
    "            'r2': current_r2,\n",
    "            'r2_improvement': r2_improvement\n",
    "        })\n",
    "        \n",
    "        \n",
    "        # Stop if the model is very confident OR if it is moderately confident and not improving.\n",
    "        if (max_std < uncertainty_threshold) or ((max_std < higher_uncertainty_threshold) and (stagnant_iterations >= max_stagnant_iterations)):\n",
    "            break\n",
    "        \n",
    "        # Add the candidate with highest uncertainty to the training set\n",
    "        training_indices.add(chosen_candidate_idx)\n",
    "        candidate_indices.remove(chosen_candidate_idx)\n",
    "    \n",
    "    # Final training on the selected samples\n",
    "    X_final = X.iloc[sorted(list(training_indices))]\n",
    "    y_final = y.iloc[sorted(list(training_indices))]\n",
    "    final_model = initialize_model()\n",
    "    final_model.fit(X_final, y_final)\n",
    "    final_predictions = final_model.predict(X)\n",
    "    final_r2 = r2_score(y, final_predictions)\n",
    "    \n",
    "    return {\n",
    "        'final_model': final_model,\n",
    "        'final_predictions': final_predictions,\n",
    "        'final_r2': final_r2,\n",
    "        'iteration_log': iteration_log,\n",
    "        'total_samples_used': len(training_indices)\n",
    "    }\n",
    "\n",
    "\n",
    "results = active_sampling_loop_with_stagnant_iterations_stop(X, y, max_iterations=15, \n",
    "                                                              uncertainty_threshold=0.05, \n",
    "                                                              higher_uncertainty_threshold=0.1,\n",
    "                                                              r2_improvement_threshold=1e-4, \n",
    "                                                              max_stagnant_iterations=3)\n",
    "\n",
    "print(\"Final R² Score:\", results['final_r2'])\n",
    "print(\"Total Samples Used:\", results['total_samples_used'])\n",
    "print(\"Iteration Log:\")\n",
    "for log in results['iteration_log']:\n",
    "    print(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell we visualize the active sampling process. The first part prints a log that shows, for each iteration, the number of samples used, the maximum uncertainty, and the R² score. The chart below shows two plots:\n",
    "- **R² Evolution:** How the model's performance improves over iterations.\n",
    "- **Max Uncertainty Evolution:** How the maximum prediction uncertainty decreases as more samples are added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_iteration_log(results, total_data_count):\n",
    "    final_samples = results['total_samples_used']\n",
    "    percent_used = final_samples / total_data_count * 100\n",
    "    print(f\"Total Data Available: {total_data_count}\")\n",
    "    print(f\"Total Samples Used: {final_samples} ({percent_used:.2f}%)\")\n",
    "    print(\"\\nIteration Log:\")\n",
    "    for log in results['iteration_log']:\n",
    "        print(f\"Iteration {log['iteration']}: {log['num_samples']} samples, \"\n",
    "              f\"Max Uncertainty: {log['max_uncertainty']:.4f}, R²: {log['r2']:.4f}\")\n",
    "    print(f\"\\nFinal R² Score: {results['final_r2']:.4f}\")\n",
    "\n",
    "def plot_active_sampling_results(results, total_data_count):\n",
    "    iterations = [log['iteration'] for log in results['iteration_log']]\n",
    "    r2_values = [log['r2'] for log in results['iteration_log']]\n",
    "    uncertainties = [log['max_uncertainty'] for log in results['iteration_log']]\n",
    "    \n",
    "    final_samples = results['total_samples_used']\n",
    "    percent_used = final_samples / total_data_count * 100\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(8, 10))\n",
    "    \n",
    "    ax[0].plot(iterations, r2_values, marker='o', linestyle='-')\n",
    "    ax[0].set_title('R² Evolution Over Iterations')\n",
    "    ax[0].set_xlabel('Iteration')\n",
    "    ax[0].set_ylabel('R² Score')\n",
    "    ax[0].grid(True)\n",
    "    ax[0].text(0.5, 0.1, f'Final R²: {results[\"final_r2\"]:.4f}', \n",
    "               transform=ax[0].transAxes, fontsize=12, color='green',\n",
    "               bbox=dict(facecolor='white', alpha=0.8, edgecolor='green'))\n",
    "    \n",
    "    \n",
    "    ax[1].plot(iterations, uncertainties, marker='o', linestyle='-')\n",
    "    ax[1].set_title('Max Uncertainty Evolution Over Iterations')\n",
    "    ax[1].set_xlabel('Iteration')\n",
    "    ax[1].set_ylabel('Max Uncertainty (std)')\n",
    "    ax[1].grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Active Sampling: {final_samples} of {total_data_count} samples used ({percent_used:.2f}%)', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "total_data_count = X.shape[0]  # For example, if X has 44 rows\n",
    "\n",
    "print_iteration_log(results, total_data_count)\n",
    "plot_active_sampling_results(results, total_data_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Regression - Full dataset Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Sampling Model Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we execute our active sampling models over all parameter combinations. We provide two variants:\n",
    "- **Basic Active Sampling Loop:** The standard loop that iteratively selects samples based on maximum uncertainty.\n",
    "- **Active Sampling Loop with Stagnant Iterations Stop:** An enhanced loop that also monitors `R²` improvements and stops if the model's performance stagnates while uncertainty is moderately low.\n",
    "\n",
    "This section prepares the results for each parameter combination by running the appropriate active sampling function and collecting key metrics such as the final R², total samples used, and iteration logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_active_sampling_basic(params_list, data, sort=False, verbose=False, **hyperparams):\n",
    "    results_list = []\n",
    "    if not verbose:\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for i, param in enumerate(tqdm(params_list, desc=\"Running basic active sampling\", disable=verbose)):\n",
    "        X, y, scaler, reverse_scaling = prepare_training_data(param, data, sort=sort)\n",
    "        total_data_count = X.shape[0] \n",
    "        \n",
    "        result = active_sampling_loop(X, y, **hyperparams)\n",
    "        result['params'] = param\n",
    "        result['total_data_count'] = total_data_count\n",
    "        results_list.append(result)\n",
    "        if verbose:\n",
    "            print(f\"Completed parameter {i+1}/{len(params_list)}: R² = {result['final_r2']:.4f}, Samples = {result['total_samples_used']} of {X.shape[0]}\")\n",
    "            \n",
    "    warnings.filterwarnings(\"default\", category=ConvergenceWarning)\n",
    "\n",
    "    return results_list\n",
    "\n",
    "basic_hyperparams = {\n",
    "    \"max_iterations\": 15,\n",
    "    \"uncertainty_threshold\": 0.05,\n",
    "}\n",
    "\n",
    "results_basic = run_active_sampling_basic(\n",
    "params_mostData[:3], data_filtered, \n",
    "verbose=True,\n",
    "**basic_hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_active_sampling_stagnant(params_list, data, sort=False ,verbose=False, **hyperparams):\n",
    "    results_list = []\n",
    "    if not verbose:\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    \n",
    "    for i, param in enumerate(tqdm(params_list, desc=\"Running stagnant active sampling\", disable=verbose)):\n",
    "        X, y, scaler, reverse_scaling = prepare_training_data(param, data, sort=sort)\n",
    "        total_data_count = X.shape[0] \n",
    "        \n",
    "        result = active_sampling_loop_with_stagnant_iterations_stop(X, y, **hyperparams)\n",
    "        result['params'] = param\n",
    "        result['total_data_count'] = total_data_count \n",
    "        results_list.append(result)\n",
    "        if verbose:\n",
    "            print(f\"Completed parameter {i+1}/{len(params_list)}: R² = {result['final_r2']:.4f}, Samples = {result['total_samples_used']} of {total_data_count}\")\n",
    "    \n",
    "    warnings.filterwarnings(\"default\", category=ConvergenceWarning)\n",
    "\n",
    "    return results_list\n",
    "\n",
    "stagnant_hyperparams = {\n",
    "    \"max_iterations\": 15,\n",
    "    \"uncertainty_threshold\": 0.05,\n",
    "    \"higher_uncertainty_threshold\": 0.1,\n",
    "    \"r2_improvement_threshold\": 1e-4,\n",
    "    \"max_stagnant_iterations\": 3\n",
    "}\n",
    "\n",
    "results_stagnant = run_active_sampling_stagnant(\n",
    "params_mostData[:3], data_filtered,\n",
    "verbose =True,\n",
    "**stagnant_hyperparams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Visualization & Preliminary Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this cell, we visualize the aggregated results. The visualization function now accepts:\n",
    "- `loop_type`: A string indicating the active sampling variant (e.g., \"Basic Active Loop\" or \"Stagnant Active Loop\").\n",
    "- `hyperparams`: A dictionary of hyperparameter values used for the active sampling loop.\n",
    "\n",
    "The function then creates a title that includes these hyperparameter values and displays three bar charts:\n",
    "- Final `R²` scores.\n",
    "- Total samples used.\n",
    "- Data utilization percentages (samples used as a percentage of total available data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_aggregated_results(results_list, loop_type, hyperparams):\n",
    "    title_str = f\"{loop_type} Results\"\n",
    "    \n",
    "    hyperparams_items = [f\"{k}: {v}\" for k, v in hyperparams.items()]\n",
    "    hyperparams_str = \"\"\n",
    "    for i in range(0, len(hyperparams_items), 3):\n",
    "        chunk = \" | \".join(hyperparams_items[i:i+3])\n",
    "        hyperparams_str += chunk + (\"\\n\" if i + 3 < len(hyperparams_items) else \"\")\n",
    "    \n",
    "    final_r2_list = [r['final_r2'] for r in results_list]\n",
    "    samples_used_list = [r['total_samples_used'] for r in results_list]\n",
    "    total_data_counts = [r['total_data_count'] for r in results_list]\n",
    "    data_utilization_list = [ (used / total) * 100 for used, total in zip(samples_used_list, total_data_counts)]\n",
    "    \n",
    "    n = len(results_list)\n",
    "    indices = range(n)\n",
    "    \n",
    "    avg_r2 = sum(final_r2_list) / n\n",
    "    avg_samples = sum(samples_used_list) / n\n",
    "    avg_utilization = sum(data_utilization_list) / n\n",
    "    \n",
    "    # Create subplots for the aggregated metrics.\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Bar chart for final R².\n",
    "    axes[0].bar(indices, final_r2_list, color='skyblue')\n",
    "    axes[0].axhline(avg_r2, color='red', linestyle='--', label=f'Avg R² = {avg_r2:.4f}')\n",
    "    axes[0].set_xlabel('Parameter Combination Index')\n",
    "    axes[0].set_ylabel('Final R²')\n",
    "    axes[0].set_title('Final R² for Each Combination')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Bar chart for total samples used.\n",
    "    axes[1].bar(indices, samples_used_list, color='lightgreen')\n",
    "    axes[1].axhline(avg_samples, color='red', linestyle='--', label=f'Avg Samples = {avg_samples:.2f}')\n",
    "    axes[1].set_xlabel('Parameter Combination Index')\n",
    "    axes[1].set_ylabel('Total Samples Used')\n",
    "    axes[1].set_title('Samples Used per Combination')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Bar chart for data utilization (%).\n",
    "    axes[2].bar(indices, data_utilization_list, color='plum')\n",
    "    axes[2].axhline(avg_utilization, color='red', linestyle='--', label=f'Avg Utilization = {avg_utilization:.2f}%')\n",
    "    axes[2].set_xlabel('Parameter Combination Index')\n",
    "    axes[2].set_ylabel('Data Utilization (%)')\n",
    "    axes[2].set_title('Data Utilization per Combination')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    # Set a prominent super-title at the top.\n",
    "    plt.suptitle(title_str, fontsize=20, fontweight='bold', color='navy', y=0.98)\n",
    "    \n",
    "    # Add a centered hyperparameters container below the title.\n",
    "    # This container is placed at y=0.93 and is horizontally centered (x=0.5).\n",
    "    plt.gcf().text(0.5, 0.85, f\"Hyperparameters:\\n\\n{hyperparams_str}\", \n",
    "                   fontsize=12, ha='center', va='center',\n",
    "                   bbox=dict(facecolor='lightgrey', alpha=0.6, boxstyle='round,pad=0.5'))\n",
    "    \n",
    "    # Adjust the layout so that subplots start below the hyperparameters container.\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.83])\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average Final R²: {avg_r2:.4f}\")\n",
    "    print(f\"Average Samples Used: {avg_samples:.2f}\")\n",
    "    print(f\"Average Data Utilization: {avg_utilization:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_basic = run_active_sampling_basic(params_mostData, data_filtered, verbose=False, **basic_hyperparams)\n",
    "visualize_aggregated_results(results_basic, \"Basic Active Loop\", basic_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stagnant = run_active_sampling_stagnant(params_mostData, data_filtered, verbose=False, **stagnant_hyperparams)\n",
    "visualize_aggregated_results(results_stagnant, \"Stagnant Active Loop\", stagnant_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminary Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "\n",
    "![Basic results updated](./images/step-4-Basic-initial-results.png)\n",
    "\n",
    "![Stagnant results](./images/step-4-stagnat-initial-results.png)  \n",
    "\n",
    "**Basic V.S. Stagnant**\n",
    "\n",
    "- **Basic Active Sampling Loop:**\n",
    "  - **Robustness:** Tends to use around 18 samples per parameter combination, yielding high `R²` scores (often 1.0000).\n",
    "  - **Efficiency:** Uses more data, which may be unnecessary in cases with abundant information.\n",
    "\n",
    "- **Stagnant Active Sampling Loop:**\n",
    "  - **Efficiency:** Stops early when `R²` improvement stagnates, generally using fewer samples (between 11–16) and still achieving high performance in most cases.\n",
    "  - **Risks:** In some cases (e.g., parameter combinations with very few total data points), the loop stops after only 3 samples, leading to negative `R²` values and clear underfitting.\n",
    "\n",
    " **Outliers and Data Issues**\n",
    "\n",
    "- **Outlier Cases:**  \n",
    "  Some parameter combinations consistently use only 3 samples and report poor performance (e.g., negative `R²`). These cases likely represent scenarios where the available data is sparse or noisy, right now its looking like  the most promising direction to investigate and improve.\n",
    "\n",
    "- **Implication:**  \n",
    "  Relying solely on the stagnant loop's stopping criterion can result in premature termination. A minimum sample threshold may be necessary to ensure that the model has enough data to learn meaningful patterns.\n",
    "\n",
    " **Next Steps**\n",
    "\n",
    "1. **Revisit the Data:**  \n",
    "   Analyze the outlier parameter combinations with low total data counts. Determine if these cases should be treated differently or if more data can be acquired.\n",
    "\n",
    "2. **Implement a Minimum Sample Threshold:**  \n",
    "   Adjust the stagnant loop to enforce a minimum number of samples (e.g., at least 5 or 7 samples) before allowing early termination (probably most promising result).\n",
    "\n",
    "3. **Hyperparameter Tuning:**  \n",
    "   Experiment with different settings for the uncertainty thresholds and maximum stagnant iterations to balance efficiency and robustness. Fine-tuning may reduce instances where the model stops too early.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Optimization and Data Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, would try to improve our results and address the points raised in the [preliminary conclusions](#preliminary-conclusions)\n",
    "\n",
    "- Revisit the Data\n",
    "- Implement a Minimum Sample Threshold\n",
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Revisit Parameter Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we re-calculate the parameter combinations using our two strategies:\n",
    "- **onePerHotel:** For each hotel, we select the combination (of `Snapshot Date`, `Hotel Name`, and `Discount Code`) with the highest transaction count.\n",
    "- **mostData:** We select the top 40 overall combinations based on transaction counts.\n",
    "\n",
    "We then display these results in a table (including the transaction counts) so we can review the distribution and identify any potential outliers in data availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_filtered.groupby(\n",
    "    ['Snapshot Date', 'Hotel Name', 'Discount Code']\n",
    ").size().reset_index(name='transaction_count')\n",
    "\n",
    "onePerHotel = (\n",
    "    grouped.sort_values(['Hotel Name', 'transaction_count'], ascending=[True, False])\n",
    "    .groupby('Hotel Name')\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "onePerHotel = onePerHotel.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "mostData = grouped.sort_values('transaction_count', ascending=False).head(40)\n",
    "\n",
    "print(\"One Per Hotel (Top 40) Parameter Combinations with Transaction Counts:\")\n",
    "display(onePerHotel)\n",
    "\n",
    "print(\"\\nMost Data (Top 40) Parameter Combinations with Transaction Counts:\")\n",
    "display(mostData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks solid to me, shows again the for maximum data we rather stick to Most Data params instead of One Per Hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Investigating Valid Data and Outliers for Parameter Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we further analyze the data returned by `load_relevant_data` for each parameter combination. For each combination, we compute:\n",
    "- Total rows (number of transactions)\n",
    "- Unique counts for WeekDays, Snapshot IDs, Check-in Dates, and Available Rooms\n",
    "- Weekend count and Holiday-Connected count (using US holidays for 2014–2016, with a check for a holiday occurring within 3 days before or 1 day after the check-in date)\n",
    "- Data range (from snapshot date to snapshot date + 30 days)\n",
    "- **Price Metrics:**\n",
    "  - Minimum and maximum Discount Price\n",
    "  - Price range as a percentage\n",
    "  - An array of price points (each price point is `[price, showcount]`)\n",
    "\n",
    "We then split the parameter combinations into two groups (Good Predictions vs. Suspected Outliers) based on the final `R²` value and display an average comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "# Create a US holidays object for the years 2014-2016.\n",
    "us_holidays = holidays.US(years=[2014, 2015, 2016])\n",
    "\n",
    "def is_connected_to_holiday(checkin_date, window_before=3, window_after=1):\n",
    "    checkin_date = pd.to_datetime(checkin_date)\n",
    "    start_window = checkin_date - pd.Timedelta(days=window_before)\n",
    "    end_window = checkin_date + pd.Timedelta(days=window_after)\n",
    "    for holiday_date in us_holidays.keys():\n",
    "        holiday_ts = pd.to_datetime(holiday_date)\n",
    "        if start_window <= holiday_ts <= end_window:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "summary_metrics = []\n",
    "for result in results_stagnant:  # (Use results from the stagnant or basic loop as needed)\n",
    "    param = result['params']\n",
    "    df_subset = load_relevant_data(param, data_filtered)\n",
    "    total_rows = df_subset.shape[0]\n",
    "    \n",
    "    unique_weekdays = df_subset['WeekDay'].nunique() if 'WeekDay' in df_subset.columns else None\n",
    "    unique_snapshot_ids = df_subset['Snapshot ID'].nunique() if 'Snapshot ID' in df_subset.columns else None\n",
    "    unique_checkin_dates = df_subset['Checkin Date'].nunique() if 'Checkin Date' in df_subset.columns else None\n",
    "    unique_rooms = df_subset['Available Rooms'].nunique() if 'Available Rooms' in df_subset.columns else None\n",
    "    \n",
    "    weekend_count = df_subset[df_subset['WeekDay'].isin([\"Friday\", \"Saturday\", \"Sunday\"])].shape[0]\n",
    "    holiday_connected_count = df_subset[df_subset['Checkin Date'].apply(is_connected_to_holiday)].shape[0]\n",
    "    \n",
    "    snapshot_dt = pd.to_datetime(param['Snapshot Date'])\n",
    "    data_range = f\"{snapshot_dt.strftime('%Y-%m-%d')} - {(snapshot_dt + pd.Timedelta(days=30)).strftime('%Y-%m-%d')}\"\n",
    "    \n",
    "    # Price metrics\n",
    "    if total_rows > 0:\n",
    "        discount_price_min = df_subset[\"Discount Price\"].min()\n",
    "        discount_price_max = df_subset[\"Discount Price\"].max()\n",
    "        discount_price_mean = df_subset[\"Discount Price\"].mean()\n",
    "        discount_price_std = df_subset[\"Discount Price\"].std()\n",
    "        price_range_percent = ((discount_price_max - discount_price_min) / discount_price_min) * 100 if discount_price_min != 0 else None\n",
    "        price_counts = df_subset[\"Discount Price\"].value_counts().sort_index()\n",
    "        price_points = [[price, count] for price, count in price_counts.items()]\n",
    "    else:\n",
    "        discount_price_min = discount_price_max = discount_price_mean = discount_price_std = price_range_percent = None\n",
    "        price_points = []\n",
    "    \n",
    "    param_key = f\"{param['Snapshot Date']} | {param['Hotel Name']} | {param['Discount Code']}\"\n",
    "    \n",
    "    summary_metrics.append({\n",
    "        \"Parameter\": param_key,\n",
    "        \"Final R²\": result['final_r2'],\n",
    "        \"Total Rows\": total_rows,\n",
    "        \"Unique WeekDays\": unique_weekdays,\n",
    "        \"Unique Snapshot IDs\": unique_snapshot_ids,\n",
    "        \"Unique Checkin Dates\": unique_checkin_dates,\n",
    "        \"Unique Rooms\": unique_rooms,\n",
    "        \"Weekend Count\": weekend_count,\n",
    "        \"Holiday Connected Count\": holiday_connected_count,\n",
    "        \"Data Range\": data_range,\n",
    "        \"Price Points\": price_points,\n",
    "        \"Price Points length\": len(price_points),\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(summary_metrics)\n",
    "\n",
    "# Split into groups based on final R² (good predictions vs. suspected outliers)\n",
    "df_predicting_well = df_metrics[df_metrics[\"Final R²\"] >= 0]\n",
    "df_outliers = df_metrics[df_metrics[\"Final R²\"] < 0]\n",
    "\n",
    "# Compute averages for numeric columns (price metrics will be averaged only for percentage metrics)\n",
    "avg_good = df_predicting_well.mean(numeric_only=True)\n",
    "avg_bad = df_outliers.mean(numeric_only=True)\n",
    "\n",
    "avg_table = pd.DataFrame({\"Good Predictions\": avg_good, \"Bad Predictions\": avg_bad})\n",
    "print(\"Average Metrics Comparison:\")\n",
    "display(avg_table)\n",
    "\n",
    "print(\"\\nDetailed Metrics for Good Predictions:\")\n",
    "display(df_predicting_well)\n",
    "print(\"\\nDetailed Metrics for Suspected Outliers:\")\n",
    "display(df_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " **Average Metrics Comparison**\n",
    "\n",
    "| Metric                      | Good Predictions | Bad Predictions  |\n",
    "|-----------------------------|------------------|------------------|\n",
    "| Final R²                    | 0.999862         | -0.603804        |\n",
    "| Total Rows                  | 27.677419        | 24.888889        |\n",
    "| Unique WeekDays             | 6.419355         | 7.000000         |\n",
    "| Unique Snapshot IDs         | 1.354839         | 1.111111         |\n",
    "| Unique Checkin Dates        | 22.580645        | 23.666667        |\n",
    "| Unique Rooms                | 5.225806         | 9.333333         |\n",
    "| Weekend Count               | 9.741935         | 11.000000        |\n",
    "| Holiday Connected Count     | 2.548387         | 4.111111         |\n",
    "| Price Points Length         | 6.419355         | 2.888889         |\n",
    "\n",
    "*Note: The current dataset used for model training did not include the \"holiday connected\" feature. The holiday connected count is computed solely for this analysis.*\n",
    "\n",
    "**Key Observations**\n",
    "\n",
    "- **Overall Quality:**  \n",
    "  - **Similarities:**  \n",
    "  Both groups have comparable totals for rows, unique weekdays, snapshot IDs, and check-in dates.\n",
    "\n",
    "  - **Differences:**  \n",
    "  The outlier group shows notably higher variability in unique rooms (9.33 vs. 5.23) and a higher holiday connected count (4.11 vs. 2.55).\n",
    "\n",
    "- **Outlier Behavior: Two Subgroups **  \n",
    "  1. **The Carlyle A Rosewood Group:**  \n",
    "   - **Performance:** Final R² values are very low or negative.  \n",
    "   - **Price Data:** Limited price variability (there are only unique 2 price points per data set), with many entries concentrated at one price.  \n",
    "   - **Other Metrics:** Higher unique rooms and holiday connected counts suggest inconsistencies in the data for this hotel.\n",
    "\n",
    "  2. **The New York EDITION and Park Hyatt New York Group:**  \n",
    "    - **Performance:** Despite being flagged as outliers, these cases exhibit high final R² (good model performance).  \n",
    "    - **Price Data:** They display greater diversity in price points and more consistent room data, which might be influenced by market factors not captured in the current model.\n",
    "\n",
    "\n",
    "\n",
    "**Next Steps and Conclusions**\n",
    "\n",
    "- The overall data quality appears acceptable. Although adding the holiday connected feature is an option, my intuition is that it may not significantly improve the model—and could potentially add more noise.\n",
    "- The limited price variability in the Carlyle A Rosewood subgroup is a promising direction for further investigation.\n",
    "- The immediate next step should be to examine the algorithm's stopping criteria. In particular, we need to understand why the active sampling stops after only 3 requests and explore the effects of modifying this behavior to request more data points before termination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we stop? (Minimum Sample Threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: \n",
    "- Why did the model stop sampling after 3 iterations? \n",
    "- what would happen if we set a minimum number of iterations (higher)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debugging the Stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we investigate why the basic active sampling loop stops after only a few iterations. We focus on three parameter combinations from the suspected outlier group:\n",
    "- **params_mostData[9]** (The New York EDITION)\n",
    "- **params_mostData[21]** (The Carlyle A Rosewood)\n",
    "- **params_mostData[29]** (Park Hyatt New York)\n",
    "\n",
    "We use a debug version of the active sampling loop that prints:\n",
    "- The training and candidate indices at each iteration.\n",
    "- The uncertainty values for each candidate.\n",
    "- The final training Data.\n",
    "- The maximum uncertainty and the chosen candidate index.\n",
    "- The current `R²` score.\n",
    "\n",
    "This information should shed light on why the loop is stopping early, helping us determine if a higher minimum sample threshold might be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling_loop_debug_table(X, y, max_iterations=10, uncertainty_threshold=0.05, initial_sample_indices=None):\n",
    "    n_samples = X.shape[0]\n",
    "    if initial_sample_indices is None:\n",
    "        initial_sample_indices = [0, n_samples // 2, n_samples - 1]\n",
    "    \n",
    "    training_indices = set(initial_sample_indices)\n",
    "    candidate_indices = set(range(n_samples)) - training_indices\n",
    "    iteration_log = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        X_train = X.iloc[sorted(list(training_indices))]\n",
    "        y_train = y.iloc[sorted(list(training_indices))]\n",
    "\n",
    "        model = initialize_model()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        candidate_list = sorted(list(candidate_indices))\n",
    "        X_candidates = X.iloc[candidate_list]\n",
    "        y_pred_candidates, std_candidates = model.predict(X_candidates, return_std=True)\n",
    "        \n",
    "        max_std = np.max(std_candidates)\n",
    "        chosen_candidate_idx = candidate_list[np.argmax(std_candidates)]\n",
    "        \n",
    "        y_pred_full = model.predict(X)\n",
    "        current_r2 = r2_score(y, y_pred_full)\n",
    "        \n",
    "        iteration_details = {\n",
    "            \"Iteration\": iteration,\n",
    "            \"Training Indices\": \", \".join(map(str, sorted(training_indices))),\n",
    "            \"Candidate Indices\": \", \".join(map(str, sorted(candidate_indices))),\n",
    "            \"Candidate Uncertainties\": str(np.round(std_candidates, 4).tolist()),\n",
    "            \"Max Uncertainty\": round(max_std, 4),\n",
    "            \"Chosen Candidate\": chosen_candidate_idx,\n",
    "            \"Current R²\": round(current_r2, 4)\n",
    "        }\n",
    "        iteration_log.append(iteration_details)\n",
    "        \n",
    "        if max_std < uncertainty_threshold:\n",
    "            break\n",
    "        \n",
    "        training_indices.add(chosen_candidate_idx)\n",
    "        candidate_indices.remove(chosen_candidate_idx)\n",
    "    \n",
    "    X_final = X.iloc[sorted(list(training_indices))]\n",
    "    y_final = y.iloc[sorted(list(training_indices))]\n",
    "    final_model = initialize_model()\n",
    "    final_model.fit(X_final, y_final)\n",
    "    final_predictions = final_model.predict(X)\n",
    "    final_r2 = r2_score(y, final_predictions)\n",
    "    \n",
    "    final_details = {\n",
    "        \"Final R²\": round(final_r2, 4),\n",
    "        \"Total Samples Used\": len(training_indices)\n",
    "    }\n",
    "    \n",
    "    print(\"Iteration Details:\")\n",
    "    display(pd.DataFrame(iteration_log))\n",
    "    print(\"\\nFinal Details:\")\n",
    "    display(pd.DataFrame([final_details]))\n",
    "    \n",
    "    print(\"\\nFinal Training Data (first 10 rows):\")\n",
    "    display(X_final.head(10))\n",
    "    print(\"Final Training Target (first 10 rows):\")\n",
    "    display(y_final.head(10))\n",
    "    \n",
    "    return {\n",
    "        'final_model': final_model,\n",
    "        'final_predictions': final_predictions,\n",
    "        'final_r2': final_r2,\n",
    "        'iteration_log': iteration_log,\n",
    "        'total_samples_used': len(training_indices)\n",
    "    }\n",
    "\n",
    "# Debug the basic loop for selected parameters from the suspected outlier group.\n",
    "params_to_debug = [params_mostData[9], params_mostData[21], params_mostData[29]]\n",
    "\n",
    "for param in params_to_debug:\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Debugging for parameter: {param['Snapshot Date']} | {param['Hotel Name']} | {param['Discount Code']}\")\n",
    "    X_debug, y_debug, scaler, reverse_scaling = prepare_training_data(param, data_filtered)\n",
    "    result_debug = active_sampling_loop_debug_table(X_debug, y_debug, max_iterations=15, uncertainty_threshold=0.05)\n",
    "    print(\"--------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis and Improvements: Understanding R² and Model Behavior**\n",
    "\n",
    " **What We Found**\n",
    "- **Initial Sample Homogeneity:**  \n",
    "  We observed that the three initial samples (selected using indices `[0, n_samples // 2, n_samples - 1]`) in out outliers cased often turn out to be very similar. having the same discount price. \n",
    "  - This causes the model to quickly learn a prediction near initial indices discount price.\n",
    "\n",
    " **A Reminder About R²**\n",
    "- **R² (Coefficient of Determination):**  \n",
    "  Measures how well the model explains the variance in the target variable.\n",
    "  - **R² ≈ 1:** The model explains nearly all the variability.\n",
    "  - **R² ≈ 0:** The model performs no better than simply predicting the mean.\n",
    "  - **Negative R²:** The model performs worse than predicting the mean.\n",
    "\n",
    " **Illustrative Examples**\n",
    "1. **Example 1:** `[1, 3, 5, 7, 1, 8, 4, 7, 1]`\n",
    "   - **Average Calculation:**  \n",
    "     Sum = 37, Average ≈ 4.11  \n",
    "   - **Scenario:**  \n",
    "     If the model starts with three initial samples that are all `1`, it learns that the target is about 1.  \n",
    "     However, because the overall average is 4.11, predicting 1 is far off, resulting in a very poor model fit (negative R²).\n",
    "\n",
    "2. **Example 2:** `[1, 1, 1, 1, 1, 1, 1, 3, 1]`\n",
    "   - **Average Calculation:**  \n",
    "     Sum = 11, Average ≈ 1.22  \n",
    "   - **Scenario:**  \n",
    "     If the model again starts with three `1`s, it learns that the target is about 1.  \n",
    "     Here, because the overall average is close to 1 (1.22), the prediction is nearly as good as the average, leading to an R² close to 0.\n",
    "\n",
    "These examples illustrate that when initial samples are too similar, the model essentially learns to predict a single value—its prediction is then almost equivalent to the mean, causing R² to be very low or even negative.\n",
    "\n",
    " **How to Improve the Model**\n",
    "**Potential Strategies:**\n",
    "- **Quantile-Based (Sorted) Sampling:**  \n",
    "  - *Pros:* Ensures that the initial samples cover the full range of discount prices.\n",
    "  - *Cons:* Requires additional preprocessing to determine quantile indices. im most cases this would lead to a trade of with sampling amount.\n",
    "- **Increasing the Initial Sample Count:**  \n",
    "  - *Pros:* Captures more variability by starting with more diverse data points.\n",
    "  - *Cons:* Increases computational cost in early iterations + may not solve the problem in all cases (see example 2).\n",
    "- **Random or Stratified Sampling:**  \n",
    "  - *Pros:* Reduces bias and ensures different segments of the data are represented.\n",
    "  - *Cons:* May yield inconsistent results unless a fixed random seed is used + may not solve the problem in all cases (see example 2).\n",
    "\n",
    "**Current Approach:**  \n",
    "We currently sort the sub-dataset by discount price so that the initial samples are drawn from different parts of the distribution. However, when the data are naturally homogeneous, this method may still select very similar points.\n",
    "\n",
    " **Summary**\n",
    "When the initial sample points are too similar, the model essentially learns to predict a value near the average, leading to an R² near 0 or even negative scores. In contrast, a more diverse set of initial samples could improve the model’s ability to capture the data’s variability. Our next steps will focus on refining the sampling strategy—possibly using quantile-based or stratified sampling—and exploring a higher minimum sample threshold to achieve better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the data and visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add the sort option to `prepare_training_data`, if sort=true the data will be sorted by discount price.\n",
    "```py\n",
    "\n",
    "if sorted:\n",
    "        normalized = normalized.sort_values(by=\"Discount Price\")\n",
    "\n",
    "```\n",
    "\n",
    "and added the same option to `run_active_sampling_stagnant` and `run_active_sampling_basic`.\n",
    "\n",
    "lets see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_basic = run_active_sampling_basic(params_mostData, data_filtered, sort=True, verbose=False, **basic_hyperparams)\n",
    "visualize_aggregated_results(results_basic, \"Basic - sorted\", basic_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_stagnant = run_active_sampling_stagnant(params_mostData, data_filtered, sort=True ,verbose=False, **stagnant_hyperparams)\n",
    "visualize_aggregated_results(results_stagnant, \"Stagnant - sorted\", stagnant_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic results](./images/step4-basic-sorted-Result(pre-tuning).png)\n",
    "![stagnat results](./images/step4-stagnat-sorted-Result(pre-tuning).png)\n",
    "\n",
    "**Wow!** This improvement is insane! but the data utilization is not so great, 67% percent for basic and 48% for stagnant, we can do better!\n",
    "Lets fine tune the hyper parameters to lose some performance for less trining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning hyperparameters  to get the best R^2 to Data utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to strike the right balance between model performance (as measured by R²) and data efficiency (minimizing the percentage of available data used).\n",
    "In our active sampling loops, the stopping criteria play a key role in this trade-off.\n",
    "\n",
    " **What We’re Tuning**\n",
    "\n",
    "**For the Basic Active Sampling Loop:**\n",
    "- **max_iterations:** The maximum number of iterations allowed.\n",
    "- **uncertainty_threshold:** The threshold below which the model’s uncertainty is considered low enough to stop sampling.\n",
    "\n",
    "**For the Stagnant Active Sampling Loop:**\n",
    "- **max_iterations:** Same as above.\n",
    "- **uncertainty_threshold:** Primary stopping threshold.\n",
    "- **higher_uncertainty_threshold:** A secondary threshold used when combined with stagnation.\n",
    "- **r2_improvement_threshold:** The minimum improvement in R² required between iterations.\n",
    "- **max_stagnant_iterations:** The maximum number of consecutive iterations with minimal R² improvement before stopping.\n",
    "\n",
    "**How would we do that?**\n",
    "\n",
    "We would use **BayesianOptimization** to fine-tune our hyperparameters because it efficiently explores the hyperparameter space by balancing exploration and exploitation. This method requires fewer iterations compared to exhaustive grid search and quickly converges to the best set of parameters that maximize our model's R².\n",
    "\n",
    "Our objective function returns the final R², and we define bounds for key hyperparameters (like uncertainty thresholds, R² improvement thresholds, and the maximum stagnant iterations). The optimized parameters help improve model performance and data utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning with Bayesian Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-Tuning Hyperparameters with Bayesian Optimization**\n",
    "\n",
    "We use Bayesian Optimization to search for the best hyperparameters for our stagnant active sampling loop. Our objective function, `objective_hyperparams_stagnant/basic`, randomly selects 5 parameter combinations from `params_mostData` (with sorted data) and runs the stagnant active sampling loop on each. It then computes the average final R² and average data utilization. A penalty is applied if the average R² falls below 0.96, ensuring that only hyperparameters yielding strong performance are favored.\n",
    "\n",
    "The optimizer then finds the hyperparameter values that maximize our objective. We save the optimized hyperparameters in a variable (`stagnant/basic_optimized_hyperparameters`) and also pickle them to disk for future use. Finally, we run the stagnant active sampling loop with these optimized parameters and visualize the results.\n",
    "\n",
    "This process ensures we balance high R² performance with low data utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pickle\n",
    "\n",
    "def objective_hyperparams_basic(uncertainty_threshold, max_iterations):\n",
    "    max_iterations = int(max_iterations)\n",
    "    hyperparams = {\"max_iterations\": max_iterations, \"uncertainty_threshold\": uncertainty_threshold}\n",
    "\n",
    "    selected_params = random.sample(params_mostData, 3)\n",
    "    results_list = run_active_sampling_basic(selected_params, data_filtered, sort=True, verbose=False, **hyperparams)\n",
    "    \n",
    "    avg_r2 = sum(result['final_r2'] for result in results_list) / len(results_list)\n",
    "    utilization_vals = [result['total_samples_used'] / result['total_data_count'] for result in results_list]\n",
    "    avg_utilization = sum(utilization_vals) / len(utilization_vals)\n",
    "    score = 1 - avg_utilization\n",
    "    if avg_r2 < 0.96:\n",
    "        penalty = (0.96 - avg_r2) * 100\n",
    "        score -= penalty\n",
    "    return score\n",
    "\n",
    "pbounds_basic = {\"uncertainty_threshold\": (0.01, 0.1), \"max_iterations\": (10, 30)}\n",
    "\n",
    "optimizer_basic = BayesianOptimization(f=objective_hyperparams_basic, pbounds=pbounds_basic, random_state=42)\n",
    "optimizer_basic.maximize(init_points=5, n_iter=150)\n",
    "\n",
    "basic_optimized_hyperparameters = {k: int(v) if k == \"max_iterations\" else v for k, v in optimizer_basic.max['params'].items()}\n",
    "\n",
    "with open(\"basic_optimized_hyperparameters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(basic_optimized_hyperparameters, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"basic_optimized_hyperparameters.pkl\", \"rb\") as f:\n",
    "    basic_optimized_hyperparameters = pickle.load(f)\n",
    "\n",
    "results_basic = run_active_sampling_basic(params_mostData, data_filtered, sort=True, verbose=False, **basic_optimized_hyperparameters)\n",
    "visualize_aggregated_results(results_basic, \"Basic - sorted - optimized \", basic_optimized_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stagnant hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from bayes_opt import BayesianOptimization\n",
    "import pickle\n",
    "\n",
    "def objective_hyperparams_stagnant(uncertainty_threshold, higher_uncertainty_threshold, r2_improvement_threshold, max_stagnant_iterations, max_iterations):\n",
    "    max_stagnant_iterations = int(max_stagnant_iterations)\n",
    "    max_iterations = int(max_iterations)\n",
    "    hyperparams = {\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"uncertainty_threshold\": uncertainty_threshold,\n",
    "        \"higher_uncertainty_threshold\": higher_uncertainty_threshold,\n",
    "        \"r2_improvement_threshold\": r2_improvement_threshold,\n",
    "        \"max_stagnant_iterations\": max_stagnant_iterations,\n",
    "    }\n",
    "    selected_params = random.sample(params_mostData, 3)\n",
    "    results_list = run_active_sampling_stagnant(selected_params, data_filtered, sort=True, verbose=False, **hyperparams)\n",
    "    avg_r2 = sum(result['final_r2'] for result in results_list) / len(results_list)\n",
    "    utilization_vals = [result['total_samples_used'] / result['total_data_count'] for result in results_list]\n",
    "    avg_utilization = sum(utilization_vals) / len(utilization_vals)\n",
    "    score = 1 - avg_utilization\n",
    "    if avg_r2 < 0.96:\n",
    "        penalty = (0.96 - avg_r2) * 100\n",
    "        score -= penalty\n",
    "    return score\n",
    "\n",
    "pbounds = {\n",
    "    \"uncertainty_threshold\": (0.01, 0.1),\n",
    "    \"higher_uncertainty_threshold\": (0.05, 0.2),\n",
    "    \"r2_improvement_threshold\": (1e-5, 1e-3),\n",
    "    \"max_stagnant_iterations\": (2, 5),\n",
    "    \"max_iterations\": (10, 30),\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_hyperparams_stagnant,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "optimizer.maximize(init_points=5, n_iter=150)\n",
    "\n",
    "stagnant_optimized_hyperparameters = {\n",
    "    k: int(v) if k in ['max_iterations', 'max_stagnant_iterations'] else v \n",
    "    for k, v in optimizer.max['params'].items()\n",
    "}\n",
    "\n",
    "with open(\"stagnant_optimized_hyperparameters.pkl\", \"wb\") as f:\n",
    "    pickle.dump(stagnant_optimized_hyperparameters, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"stagnant_optimized_hyperparameters.pkl\", \"rb\") as f:\n",
    "    stagnant_optimized_hyperparameters = pickle.load(f)\n",
    "\n",
    "results_stagnant = run_active_sampling_stagnant(params_mostData, data_filtered, sort=True ,verbose=False, **stagnant_optimized_hyperparameters)\n",
    "visualize_aggregated_results(results_stagnant, \"Stagnant - sorted - optimized\", stagnant_optimized_hyperparameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters Fine-Tuning Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fine-tuning our hyperparameters using Bayesian Optimization, we achieved impressive improvements in data utilization without sacrificing model performance.\n",
    "\n",
    "**Results visualized**\n",
    "\n",
    "**Before optimization:**\n",
    "![Basic results pre](./images/step4-basic-sorted-Result(pre-tuning).png)\n",
    "![stagnat results pre](./images/step4-stagnat-sorted-Result(pre-tuning).png)\n",
    "\n",
    "\n",
    "**After optimization:**\n",
    "![Basic results post](./images/step4-optimized-basic.png)\n",
    "![stagnat results post](./images/step4-optimized-stagnat.png)\n",
    "\n",
    "\n",
    "**Compression**\n",
    "\n",
    "| **Model**    | **Metric**           | **Before Tuning**               | **After Tuning**                | **Percentage Change**        |\n",
    "|--------------|----------------------|---------------------------------|---------------------------------|------------------------------|\n",
    "| **Stagnant** | Data Utilization     | 48.24% (~12.78 samples/run)     | 37.52% (~9.95 samples/run)         | ~25% reduction               |\n",
    "|              | R² Reduction         | 0% drop                         | ~0.13% drop (<0.0015 reduction)   | ~0.13% drop (negligible)     |\n",
    "| **Basic**    | Data Utilization     | 67.82% (~18 samples/run)      | 42.94% (~11.35 samples/run)         | ~36% reduction               |\n",
    "|              | R² Reduction         | 0% drop                         | No visible drop                | 0% change                   |\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "We are very pleased with the final results. With the tuned hyperparameters, our stagnant model now uses only about 10 samples per run to predict the entire month’s prices with nearly 100% accuracy. This level of efficiency and performance is truly remarkable and demonstrates the strength of our approach in balancing model accuracy with data efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5 - PySpark & Mllib for step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark requires a java version of 8 or 11 to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!conda install -c conda-forge openjdk=11 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config spark to work with our env java version(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "java_home_path = os.popen('dirname $(dirname $(which java))').read().strip()\n",
    "os.environ[\"JAVA_HOME\"] = java_home_path\n",
    "print(f\"JAVA_HOME is set to: {os.environ['JAVA_HOME']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%pip install pyspark plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit, concat, first, array, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.sql.functions import min as spark_min\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HotelClustering\").getOrCreate()\n",
    "print(\"Create Spark session\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the CSV file.\n",
    "- Select the top 150 hotels (by number of records).\n",
    "- Select the top 40 checkin dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./hotels_data_changed.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "top150_hotels = df.groupBy(\"Hotel Name\").agg(count(\"*\").alias(\"cnt\")) .orderBy(col(\"cnt\").desc()).limit(150)\n",
    "\n",
    "\n",
    "df_top150 = df.join(top150_hotels.select(\"Hotel Name\"), on=\"Hotel Name\", how=\"inner\")\n",
    "\n",
    "\n",
    "top40_dates = df_top150.groupBy(\"Checkin Date\").agg(count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(40)\n",
    "\n",
    "df_top150_dates = df_top150.join(top40_dates.select(\"Checkin Date\"), on=\"Checkin Date\", how=\"inner\")\n",
    "\n",
    "df_top150_dates.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to 160-dims vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_top150_dates.groupBy(\"Hotel Name\", \"Checkin Date\", \"Discount Code\").agg(spark_min(\"Discount Price\").alias(\"minDiscountPrice\"))\n",
    "\n",
    "df_grouped = df_grouped.withColumn(\"date_code\", concat(col(\"Checkin Date\"), lit(\"_\"), col(\"Discount Code\")))\n",
    "\n",
    "df_pivot = df_grouped.groupBy(\"Hotel Name\").pivot(\"date_code\").agg(first(\"minDiscountPrice\"))\n",
    "\n",
    "df_pivot = df_pivot.fillna(-1)\n",
    "\n",
    "df_pivot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might find ourself in a situation where some of the hotels don't have a column for all the 160 dates + discount codes.\n",
    "So we would ensure thy all have that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top40_list = [row[\"Checkin Date\"] for row in top40_dates.collect()]\n",
    "discount_codes = [1, 2, 3, 4]\n",
    "\n",
    "# Build the expected column names (format: \"YYYY-MM-DD_1\", etc.)\n",
    "expected_cols = [f\"{date}_{code}\" for date in top40_list for code in discount_codes]\n",
    "\n",
    "# Add any missing expected columns with default -1\n",
    "existing_cols = df_pivot.columns\n",
    "for col_name in expected_cols:\n",
    "    if col_name not in existing_cols:\n",
    "        df_pivot = df_pivot.withColumn(col_name, lit(-1))\n",
    "\n",
    "# Reorder the DataFrame columns so that they appear in the desired order:\n",
    "df_pivot = df_pivot.select([\"Hotel Name\"] + expected_cols)\n",
    "df_pivot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Save to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the 160 price columns row-by-row (scaling valid prices to a 0–100 range, leaving missing values as -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 160 price columns into an array column\n",
    "df_pivot = df_pivot.withColumn(\"prices_array\", array(*expected_cols))\n",
    "\n",
    "# Define a UDF for normalizing the prices for each hotel (ignoring -1 values)\n",
    "def normalize_prices(prices):\n",
    "    # Filter out missing values (-1)\n",
    "    valid_prices = [p for p in prices if p != -1]\n",
    "    if not valid_prices:\n",
    "        return prices\n",
    "    min_price = min(valid_prices)\n",
    "    max_price = max(valid_prices)\n",
    "    if min_price == max_price:\n",
    "        return [0 if p != -1 else -1 for p in prices]\n",
    "    normalized = []\n",
    "    for p in prices:\n",
    "        if p == -1:\n",
    "            normalized.append(-1)\n",
    "        else:\n",
    "            norm_val = round(((p - min_price) / (max_price - min_price)) * 100)\n",
    "            normalized.append(int(norm_val))\n",
    "    return normalized\n",
    "\n",
    "normalize_udf = udf(normalize_prices, ArrayType(IntegerType()))\n",
    "\n",
    "# Apply the normalization UDF to create a new column with normalized prices.\n",
    "df_pivot = df_pivot.withColumn(\"norm_prices_array\", normalize_udf(\"prices_array\"))\n",
    "\n",
    "# Replace the original price columns with the normalized values.\n",
    "for i, col_name in enumerate(expected_cols):\n",
    "    df_pivot = df_pivot.withColumn(col_name, col(\"norm_prices_array\")[i])\n",
    "\n",
    "# Optionally, drop helper columns.\n",
    "df_final = df_pivot.drop(\"prices_array\", \"norm_prices_array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_hotels_clustering_data= \"./pyspark_hotels_clustering_data.csv\"\n",
    "df_final.write.option(\"header\", \"true\").mode(\"overwrite\").csv(pyspark_hotels_clustering_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV and assemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the path to the CSV folder that was saved in Step 5.\n",
    "pyspark_hotels_clustering_data = \"./pyspark_hotels_clustering_data.csv\"\n",
    "\n",
    "# Read the CSV data (Spark will read all part files in the folder)\n",
    "df_loaded = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(pyspark_hotels_clustering_data)\n",
    "\n",
    "# Identify the feature columns (all columns except \"Hotel Name\")\n",
    "feature_cols = [col for col in df_loaded.columns if col != \"Hotel Name\"]\n",
    "\n",
    "# Assemble the 160 normalized price columns into a single feature vector.\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_features = assembler.transform(df_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with MLlib (BisectingKMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Set the number of clusters (adjust k as needed)\n",
    "bkmeans = BisectingKMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=4)\n",
    "\n",
    "# Train the model\n",
    "model = bkmeans.fit(df_features)\n",
    "\n",
    "# Add the cluster assignments to the DataFrame\n",
    "df_clustered = model.transform(df_features)\n",
    "\n",
    "# Show the hotel names along with their cluster assignments\n",
    "df_clustered.select(\"Hotel Name\", \"cluster\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with PCA and Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are 160-dimensional, we use PCA to reduce them to 2 dimensions for visualization. Then, we convert the Spark DataFrame to a Pandas DataFrame and use Plotly Express to create a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(df_clustered)\n",
    "df_pca = pca_model.transform(df_clustered)\n",
    "\n",
    "pandas_df = df_pca.select(\"Hotel Name\", \"cluster\", \"pcaFeatures\").toPandas()\n",
    "\n",
    "# Split the PCA features into two separate columns for plotting\n",
    "pandas_df[\"pca1\"] = pandas_df[\"pcaFeatures\"].apply(lambda x: x[0])\n",
    "pandas_df[\"pca2\"] = pandas_df[\"pcaFeatures\"].apply(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    pandas_df,\n",
    "    x=\"pca1\",\n",
    "    y=\"pca2\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"Hotel Name\"],\n",
    "    title=\"Hotel Clusters Visualization (PCA Reduced)\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-101-Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
