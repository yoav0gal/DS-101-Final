{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5 - PySpark & Mllib for step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark requires a java version of 8 or 11 to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%conda install -c conda-forge openjdk=11 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config spark to work with our env java version(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "java_home_path = os.popen('dirname $(dirname $(which java))').read().strip()\n",
    "os.environ[\"JAVA_HOME\"] = java_home_path\n",
    "print(f\"JAVA_HOME is set to: {os.environ['JAVA_HOME']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%pip install pyspark plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit, concat, first, array, udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.sql.functions import min as spark_min\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HotelClustering\").getOrCreate()\n",
    "print(\"Create Spark session\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the CSV file.\n",
    "- Select the top 150 hotels (by number of records).\n",
    "- Select the top 40 checkin dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./hotels_data_changed.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "top150_hotels = df.groupBy(\"Hotel Name\").agg(count(\"*\").alias(\"cnt\")) .orderBy(col(\"cnt\").desc()).limit(150)\n",
    "\n",
    "\n",
    "df_top150 = df.join(top150_hotels.select(\"Hotel Name\"), on=\"Hotel Name\", how=\"inner\")\n",
    "\n",
    "\n",
    "top40_dates = df_top150.groupBy(\"Checkin Date\").agg(count(\"*\").alias(\"cnt\")).orderBy(col(\"cnt\").desc()).limit(40)\n",
    "\n",
    "df_top150_dates = df_top150.join(top40_dates.select(\"Checkin Date\"), on=\"Checkin Date\", how=\"inner\")\n",
    "\n",
    "df_top150_dates.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to 160-dims vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df_top150_dates.groupBy(\"Hotel Name\", \"Checkin Date\", \"Discount Code\").agg(spark_min(\"Discount Price\").alias(\"minDiscountPrice\"))\n",
    "\n",
    "df_grouped = df_grouped.withColumn(\"date_code\", concat(col(\"Checkin Date\"), lit(\"_\"), col(\"Discount Code\")))\n",
    "\n",
    "df_pivot = df_grouped.groupBy(\"Hotel Name\").pivot(\"date_code\").agg(first(\"minDiscountPrice\"))\n",
    "\n",
    "df_pivot = df_pivot.fillna(-1)\n",
    "\n",
    "df_pivot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might find ourself in a situation where some of the hotels don't have a column for all the 160 dates + discount codes.\n",
    "So we would ensure thy all have that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top40_list = [row[\"Checkin Date\"] for row in top40_dates.collect()]\n",
    "discount_codes = [1, 2, 3, 4]\n",
    "\n",
    "# Build the expected column names (format: \"YYYY-MM-DD_1\", etc.)\n",
    "expected_cols = [f\"{date}_{code}\" for date in top40_list for code in discount_codes]\n",
    "\n",
    "# Add any missing expected columns with default -1\n",
    "existing_cols = df_pivot.columns\n",
    "for col_name in expected_cols:\n",
    "    if col_name not in existing_cols:\n",
    "        df_pivot = df_pivot.withColumn(col_name, lit(-1))\n",
    "\n",
    "# Reorder the DataFrame columns so that they appear in the desired order:\n",
    "df_pivot = df_pivot.select([\"Hotel Name\"] + expected_cols)\n",
    "df_pivot.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Save to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the 160 price columns row-by-row (scaling valid prices to a 0â€“100 range, leaving missing values as -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 160 price columns into an array column\n",
    "df_pivot = df_pivot.withColumn(\"prices_array\", array(*expected_cols))\n",
    "\n",
    "# Define a UDF for normalizing the prices for each hotel (ignoring -1 values)\n",
    "def normalize_prices(prices):\n",
    "    # Filter out missing values (-1)\n",
    "    valid_prices = [p for p in prices if p != -1]\n",
    "    if not valid_prices:\n",
    "        return prices\n",
    "    min_price = min(valid_prices)\n",
    "    max_price = max(valid_prices)\n",
    "    if min_price == max_price:\n",
    "        return [0 if p != -1 else -1 for p in prices]\n",
    "    normalized = []\n",
    "    for p in prices:\n",
    "        if p == -1:\n",
    "            normalized.append(-1)\n",
    "        else:\n",
    "            norm_val = round(((p - min_price) / (max_price - min_price)) * 100)\n",
    "            normalized.append(int(norm_val))\n",
    "    return normalized\n",
    "\n",
    "normalize_udf = udf(normalize_prices, ArrayType(IntegerType()))\n",
    "\n",
    "# Apply the normalization UDF to create a new column with normalized prices.\n",
    "df_pivot = df_pivot.withColumn(\"norm_prices_array\", normalize_udf(\"prices_array\"))\n",
    "\n",
    "# Replace the original price columns with the normalized values.\n",
    "for i, col_name in enumerate(expected_cols):\n",
    "    df_pivot = df_pivot.withColumn(col_name, col(\"norm_prices_array\")[i])\n",
    "\n",
    "# Optionally, drop helper columns.\n",
    "df_final = df_pivot.drop(\"prices_array\", \"norm_prices_array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_hotels_clustering_data= \"./pyspark_hotels_clustering_data.csv\"\n",
    "df_final.write.option(\"header\", \"true\").mode(\"overwrite\").csv(pyspark_hotels_clustering_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV and assemble features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the path to the CSV folder that was saved in Step 5.\n",
    "pyspark_hotels_clustering_data = \"./pyspark_hotels_clustering_data.csv\"\n",
    "\n",
    "# Read the CSV data (Spark will read all part files in the folder)\n",
    "df_loaded = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(pyspark_hotels_clustering_data)\n",
    "\n",
    "# Identify the feature columns (all columns except \"Hotel Name\")\n",
    "feature_cols = [col for col in df_loaded.columns if col != \"Hotel Name\"]\n",
    "\n",
    "# Assemble the 160 normalized price columns into a single feature vector.\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_features = assembler.transform(df_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering with MLlib (BisectingKMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Set the number of clusters (adjust k as needed)\n",
    "bkmeans = BisectingKMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=4)\n",
    "\n",
    "# Train the model\n",
    "model = bkmeans.fit(df_features)\n",
    "\n",
    "# Add the cluster assignments to the DataFrame\n",
    "df_clustered = model.transform(df_features)\n",
    "\n",
    "# Show the hotel names along with their cluster assignments\n",
    "df_clustered.select(\"Hotel Name\", \"cluster\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization with PCA and Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are 160-dimensional, we use PCA to reduce them to 2 dimensions for visualization. Then, we convert the Spark DataFrame to a Pandas DataFrame and use Plotly Express to create a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "pca_model = pca.fit(df_clustered)\n",
    "df_pca = pca_model.transform(df_clustered)\n",
    "\n",
    "pandas_df = df_pca.select(\"Hotel Name\", \"cluster\", \"pcaFeatures\").toPandas()\n",
    "\n",
    "# Split the PCA features into two separate columns for plotting\n",
    "pandas_df[\"pca1\"] = pandas_df[\"pcaFeatures\"].apply(lambda x: x[0])\n",
    "pandas_df[\"pca2\"] = pandas_df[\"pcaFeatures\"].apply(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    pandas_df,\n",
    "    x=\"pca1\",\n",
    "    y=\"pca2\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"Hotel Name\"],\n",
    "    title=\"Hotel Clusters Visualization (PCA Reduced)\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-101-Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
